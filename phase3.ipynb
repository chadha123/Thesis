{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pit9ZM3JFhyX"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U diffusers transformers huggingface_hub accelerate sentencepiece\n",
        "!pip install -q lpips scikit-image pandas matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/thesis2\"\n",
        "\n",
        "PHASE3_ROOT = f\"{ROOT}/phase3_clip_faces\"\n",
        "IMG_ROOT = f\"{PHASE3_ROOT}/images\"\n",
        "LOG_ROOT = f\"{PHASE3_ROOT}/logs\"\n",
        "\n",
        "os.makedirs(IMG_ROOT, exist_ok=True)\n",
        "os.makedirs(LOG_ROOT, exist_ok=True)\n",
        "\n",
        "print(\"Phase 3 root:\", PHASE3_ROOT)\n",
        "print(\"Images:\", IMG_ROOT)\n",
        "print(\"Logs:\", LOG_ROOT)\n"
      ],
      "metadata": {
        "id": "7K1R38_EFwqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from diffusers import StableDiffusion3Pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "model_id = \"stabilityai/stable-diffusion-3.5-medium\"\n",
        "\n",
        "pipe = StableDiffusion3Pipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "pipe = pipe.to(\"cuda\")\n",
        "pipe.enable_attention_slicing()\n",
        "\n",
        "print(\"Loaded SD 3.5 on:\", torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "id": "wYRxxm-XF_WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "\n",
        "clip_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "clip_model_name = \"openai/clip-vit-large-patch14\"\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name).to(clip_device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "clip_model.eval()\n",
        "print(\"CLIP device:\", clip_device)\n"
      ],
      "metadata": {
        "id": "WVNa4Hx5GSqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import lpips\n",
        "\n",
        "lpips_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "lpips_model = lpips.LPIPS(net='vgg').to(lpips_device)\n",
        "lpips_model.eval()\n",
        "\n",
        "def load_image(path, size=None):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    if size is not None:\n",
        "        img = img.resize(size, Image.BICUBIC)\n",
        "    return img\n",
        "\n",
        "def img_to_numpy(img):\n",
        "    return np.asarray(img).astype(np.float32) / 255.0\n",
        "\n",
        "def img_to_lpips_tensor(img):\n",
        "    arr = np.asarray(img).astype(np.float32) / 255.0\n",
        "    arr = (arr * 2.0) - 1.0           # [-1, 1]\n",
        "    arr = torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(0)\n",
        "    return arr.to(lpips_device)\n"
      ],
      "metadata": {
        "id": "jFGGmKX4Hm2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "ANCHORS = [\n",
        "    \"neutral face portrait\",\n",
        "    \"smiling face portrait\",\n",
        "    \"serious face portrait\",\n",
        "    \"face looking confident portrait\",\n",
        "    \"face looking surprised portrait\",\n",
        "]\n",
        "\n",
        "def get_clip_image_embeddings(images_pil):\n",
        "    \"\"\"Return [N, D] tensor of normalized CLIP image embeddings.\"\"\"\n",
        "    batch_size = 4\n",
        "    feats = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(images_pil), batch_size):\n",
        "            batch = images_pil[i:i+batch_size]\n",
        "            inputs = clip_processor(\n",
        "                images=batch,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(clip_device)\n",
        "\n",
        "            out = clip_model.get_image_features(**inputs)\n",
        "            out = F.normalize(out, p=2, dim=-1)     # L2-normalize\n",
        "            feats.append(out)\n",
        "\n",
        "    return torch.cat(feats, dim=0)   # [N, D]\n",
        "\n",
        "def get_clip_text_anchor_embeddings(anchor_texts=ANCHORS):\n",
        "    with torch.no_grad():\n",
        "        inputs = clip_processor(\n",
        "            text=anchor_texts,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(clip_device)\n",
        "\n",
        "        text_feats = clip_model.get_text_features(**inputs)\n",
        "        text_feats = F.normalize(text_feats, p=2, dim=-1)   # [A, D]\n",
        "    return text_feats\n"
      ],
      "metadata": {
        "id": "ikbrj0jZHtP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchor_embs = get_clip_text_anchor_embeddings(ANCHORS)\n",
        "print(\"Anchor CLIP embedding shape:\", anchor_embs.shape)\n"
      ],
      "metadata": {
        "id": "9N-26PMRHvvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Prompts for Phase 3: neutral / smile ===\n",
        "BASE_FACE_PROMPT = (\n",
        "    \"a photorealistic portrait of a human face, studio lighting, \"\n",
        "    \"high resolution, natural skin texture, realistic anatomy, \"\n",
        "    \"professional photography, symmetric face, looking forward\"\n",
        ")\n",
        "\n",
        "CLIP_PROMPTS = {\n",
        "    \"neutral\": BASE_FACE_PROMPT + \", neutral expression, no smile, relaxed mouth, closed lips\",\n",
        "    \"smiling\": BASE_FACE_PROMPT + \", smiling, visible teeth, joyful expression, warm smile\",\n",
        "}\n",
        "\n",
        "N_CLIP_VARIANTS = 30\n",
        "BASE_SEED = 123\n",
        "HEIGHT = 768\n",
        "WIDTH = 768\n",
        "NUM_STEPS = 30\n",
        "GUIDANCE_SCALE = 7.0   # fixed for this phase\n"
      ],
      "metadata": {
        "id": "RITI1mzHHyG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "def get_clip_paths(condition: str):\n",
        "    cond_img_dir = os.path.join(IMG_ROOT, condition)\n",
        "    cond_log_path = os.path.join(LOG_ROOT, f\"clip_diversity_{condition}.csv\")\n",
        "    os.makedirs(cond_img_dir, exist_ok=True)\n",
        "    return cond_img_dir, cond_log_path\n",
        "\n",
        "def generate_clip_diversity_images(\n",
        "    condition: str,\n",
        "    n_variants: int = N_CLIP_VARIANTS,\n",
        "    base_seed: int = BASE_SEED,\n",
        "    guidance_scale: float = GUIDANCE_SCALE,\n",
        "    num_inference_steps: int = NUM_STEPS,\n",
        "    height: int = HEIGHT,\n",
        "    width: int = WIDTH,\n",
        "):\n",
        "    assert condition in CLIP_PROMPTS\n",
        "    prompt = CLIP_PROMPTS[condition]\n",
        "    cond_img_dir, cond_log_path = get_clip_paths(condition)\n",
        "\n",
        "    # init CSV\n",
        "    if not os.path.exists(cond_log_path):\n",
        "        with open(cond_log_path, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                \"timestamp\",\n",
        "                \"condition\",\n",
        "                \"prompt\",\n",
        "                \"seed\",\n",
        "                \"guidance_scale\",\n",
        "                \"num_inference_steps\",\n",
        "                \"height\",\n",
        "                \"width\",\n",
        "                \"image_path\",\n",
        "            ])\n",
        "\n",
        "    for i in range(n_variants):\n",
        "        seed = base_seed + i\n",
        "        gen = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "\n",
        "        print(f\"[{condition}] {i+1}/{n_variants} (seed={seed})\")\n",
        "        image = pipe(\n",
        "            prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            height=height,\n",
        "            width=width,\n",
        "            generator=gen,\n",
        "        ).images[0]\n",
        "\n",
        "        fname = f\"{condition}_clip_seed{seed}.png\"\n",
        "        save_path = os.path.join(cond_img_dir, fname)\n",
        "        image.save(save_path)\n",
        "\n",
        "        with open(cond_log_path, \"a\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                datetime.now().isoformat(timespec=\"seconds\"),\n",
        "                condition,\n",
        "                prompt,\n",
        "                seed,\n",
        "                guidance_scale,\n",
        "                num_inference_steps,\n",
        "                height,\n",
        "                width,\n",
        "                save_path,\n",
        "            ])\n",
        "\n",
        "        print(\" saved:\", save_path)\n",
        "\n",
        "    print(f\"✨ Done for condition='{condition}'. Log:\", cond_log_path)\n"
      ],
      "metadata": {
        "id": "fBoqx6glH0OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_clip_diversity_images(\"neutral\")\n",
        "generate_clip_diversity_images(\"smiling\")\n"
      ],
      "metadata": {
        "id": "IZXUHyOiH30O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_clip_diversity_for_condition(condition: str):\n",
        "    cond_img_dir, cond_log_path = get_clip_paths(condition)\n",
        "    print(f\"\\n=== CLIP diversity for condition: {condition} ===\")\n",
        "    print(\"Log:\", cond_log_path)\n",
        "\n",
        "    df = pd.read_csv(cond_log_path)\n",
        "    df_cond = df[df[\"condition\"] == condition].reset_index(drop=True)\n",
        "    print(\"Rows:\", len(df_cond))\n",
        "    display(df_cond[[\"seed\", \"image_path\"]])\n",
        "\n",
        "    # ---- Load images ----\n",
        "    images_pil = []\n",
        "    images_np = []\n",
        "    images_lpips_t = []\n",
        "    target_size = None\n",
        "\n",
        "    for _, row in df_cond.iterrows():\n",
        "        path = row[\"image_path\"]\n",
        "        if not os.path.exists(path):\n",
        "            print(\" missing:\", path)\n",
        "            continue\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if target_size is None:\n",
        "            target_size = img.size\n",
        "\n",
        "        img = img.resize(target_size, Image.BICUBIC)\n",
        "        images_pil.append(img)\n",
        "        images_np.append(img_to_numpy(img))\n",
        "        images_lpips_t.append(img_to_lpips_tensor(img))\n",
        "\n",
        "    print(\"Loaded\", len(images_pil), \"images of size:\", target_size)\n",
        "\n",
        "    # ---- CLIP image embeddings ----\n",
        "    image_embs = get_clip_image_embeddings(images_pil)   # [N, D]\n",
        "\n",
        "    # ---- CLIP anchor sims per image ----\n",
        "    # image_embs: [N,D], anchor_embs: [A,D] -> sims [N,A]\n",
        "    with torch.no_grad():\n",
        "        sims_img_anchor = image_embs @ anchor_embs.T     # cosine similarity\n",
        "\n",
        "    # Save image-anchor sims\n",
        "    anchor_cols = [f\"sim_anchor_{i}_{txt}\" for i, txt in enumerate(ANCHORS)]\n",
        "    anchor_cols_sanitized = [\n",
        "        c.replace(\" \", \"_\").replace(\",\", \"\").replace(\"/\", \"_\") for c in anchor_cols\n",
        "    ]\n",
        "\n",
        "    img_anchor_rows = []\n",
        "    for idx, row in df_cond.iterrows():\n",
        "        seed = int(row[\"seed\"])\n",
        "        path = row[\"image_path\"]\n",
        "        sims = sims_img_anchor[idx].cpu().numpy().tolist()\n",
        "        row_dict = {\n",
        "            \"condition\": condition,\n",
        "            \"seed\": seed,\n",
        "            \"image_path\": path,\n",
        "        }\n",
        "        for name, val in zip(anchor_cols_sanitized, sims):\n",
        "            row_dict[name] = val\n",
        "        img_anchor_rows.append(row_dict)\n",
        "\n",
        "    img_anchor_df = pd.DataFrame(img_anchor_rows)\n",
        "    img_anchor_path = os.path.join(LOG_ROOT, f\"clip_diversity_image_anchors_{condition}.csv\")\n",
        "    img_anchor_df.to_csv(img_anchor_path, index=False)\n",
        "    print(\"Saved image-anchor sims to:\", img_anchor_path)\n",
        "\n",
        "    # ---- Pairwise metrics ----\n",
        "    results = []\n",
        "    N = len(images_np)\n",
        "\n",
        "    for i in range(N):\n",
        "        for j in range(i+1, N):\n",
        "            seed_i = int(df_cond.loc[i, \"seed\"])\n",
        "            seed_j = int(df_cond.loc[j, \"seed\"])\n",
        "            path_i = df_cond.loc[i, \"image_path\"]\n",
        "            path_j = df_cond.loc[j, \"image_path\"]\n",
        "\n",
        "            arr_i = images_np[i]\n",
        "            arr_j = images_np[j]\n",
        "\n",
        "            mse_val = float(np.mean((arr_i - arr_j)**2))\n",
        "            ssim_val = float(ssim(arr_i, arr_j, channel_axis=-1, data_range=1.0))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                lpips_val = float(lpips_model(images_lpips_t[i], images_lpips_t[j]).item())\n",
        "\n",
        "            # CLIP cosine similarity / distance\n",
        "            v_i = image_embs[i]\n",
        "            v_j = image_embs[j]\n",
        "            clip_sim = float((v_i * v_j).sum().item())  # already normalized\n",
        "            clip_dist = 1.0 - clip_sim\n",
        "\n",
        "            results.append({\n",
        "                \"condition\": condition,\n",
        "                \"seed_i\": seed_i,\n",
        "                \"seed_j\": seed_j,\n",
        "                \"image_i\": path_i,\n",
        "                \"image_j\": path_j,\n",
        "                \"MSE\": mse_val,\n",
        "                \"SSIM\": ssim_val,\n",
        "                \"LPIPS\": lpips_val,\n",
        "                \"CLIP_cosine_sim\": clip_sim,\n",
        "                \"CLIP_cosine_dist\": clip_dist,\n",
        "            })\n",
        "\n",
        "    pairwise_df = pd.DataFrame(results)\n",
        "    pairwise_path = os.path.join(LOG_ROOT, f\"clip_diversity_pairwise_{condition}.csv\")\n",
        "    pairwise_df.to_csv(pairwise_path, index=False)\n",
        "    print(\"Saved pairwise metrics to:\", pairwise_path)\n",
        "\n",
        "    # ---- Summary ----\n",
        "    summary = {\n",
        "        \"condition\": condition,\n",
        "        \"N_pairs\": len(pairwise_df),\n",
        "        \"LPIPS_mean\": pairwise_df[\"LPIPS\"].mean(),\n",
        "        \"LPIPS_std\":  pairwise_df[\"LPIPS\"].std(),\n",
        "        \"SSIM_mean\":  pairwise_df[\"SSIM\"].mean(),\n",
        "        \"SSIM_std\":   pairwise_df[\"SSIM\"].std(),\n",
        "        \"MSE_mean\":   pairwise_df[\"MSE\"].mean(),\n",
        "        \"MSE_std\":    pairwise_df[\"MSE\"].std(),\n",
        "        \"CLIP_dist_mean\": pairwise_df[\"CLIP_cosine_dist\"].mean(),\n",
        "        \"CLIP_dist_std\":  pairwise_df[\"CLIP_cosine_dist\"].std(),\n",
        "        \"CLIP_sim_mean\":  pairwise_df[\"CLIP_cosine_sim\"].mean(),\n",
        "        \"CLIP_sim_std\":   pairwise_df[\"CLIP_cosine_sim\"].std(),\n",
        "    }\n",
        "    summary_df = pd.DataFrame([summary])\n",
        "\n",
        "    # Append to global summary\n",
        "    summary_path = os.path.join(LOG_ROOT, \"clip_diversity_summary.csv\")\n",
        "    if os.path.exists(summary_path):\n",
        "        old = pd.read_csv(summary_path)\n",
        "        summary_all = pd.concat([old, summary_df], ignore_index=True)\n",
        "    else:\n",
        "        summary_all = summary_df\n",
        "\n",
        "    summary_all.to_csv(summary_path, index=False)\n",
        "    print(\" Updated summary at:\", summary_path)\n",
        "    display(summary_df)\n",
        "\n",
        "    return pairwise_df, summary_df\n"
      ],
      "metadata": {
        "id": "Flb_fc--I-tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairwise_neutral_clip, summary_neutral_clip = compute_clip_diversity_for_condition(\"neutral\")\n",
        "pairwise_smiling_clip, summary_smiling_clip = compute_clip_diversity_for_condition(\"smiling\")\n"
      ],
      "metadata": {
        "id": "OJNA_AE1JCPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Tag condition in each DF\n",
        "pairwise_neutral_clip[\"condition\"] = \"neutral\"\n",
        "pairwise_smiling_clip[\"condition\"] = \"smiling\"\n",
        "pairwise_all = pd.concat([pairwise_neutral_clip, pairwise_smiling_clip], ignore_index=True)\n",
        "\n",
        "# 1) Histograms of CLIP distances per condition\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.histplot(data=pairwise_all, x=\"CLIP_cosine_dist\", hue=\"condition\", kde=True, bins=10)\n",
        "plt.title(\"CLIP cosine distance distribution (neutral vs smiling)\")\n",
        "plt.xlabel(\"CLIP cosine distance (1 - cosine similarity)\")\n",
        "plt.show()\n",
        "\n",
        "# 2) Boxplots for LPIPS / SSIM / CLIP distance\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "sns.boxplot(data=pairwise_all, x=\"condition\", y=\"LPIPS\")\n",
        "plt.title(\"LPIPS by condition\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "sns.boxplot(data=pairwise_all, x=\"condition\", y=\"SSIM\")\n",
        "plt.title(\"SSIM by condition\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "sns.boxplot(data=pairwise_all, x=\"condition\", y=\"CLIP_cosine_dist\")\n",
        "plt.title(\"CLIP distance by condition\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3) Scatter LPIPS vs CLIP distance\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.scatterplot(data=pairwise_all,\n",
        "                x=\"CLIP_cosine_dist\",\n",
        "                y=\"LPIPS\",\n",
        "                hue=\"condition\")\n",
        "plt.xlabel(\"CLIP cosine distance\")\n",
        "plt.ylabel(\"LPIPS\")\n",
        "plt.title(\"Perceptual vs semantic distance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gNFE3rfJJN-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import lpips\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n"
      ],
      "metadata": {
        "id": "1f30Wr7uObuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ROOT = \"/content/drive/MyDrive/thesis2\"\n",
        "\n",
        "FACES_ROOT = os.path.join(ROOT, \"faces_expression_intensity\")\n",
        "IMG_ROOT   = os.path.join(FACES_ROOT, \"images\")\n",
        "LOG_ROOT   = os.path.join(FACES_ROOT, \"logs\")\n",
        "\n",
        "os.makedirs(IMG_ROOT, exist_ok=True)\n",
        "os.makedirs(LOG_ROOT, exist_ok=True)\n",
        "\n",
        "print(\"FACES_ROOT:\", FACES_ROOT)\n",
        "print(\"IMG_ROOT:\", IMG_ROOT)\n",
        "print(\"LOG_ROOT:\", LOG_ROOT)\n",
        "\n",
        "# --- Conditions & prompts ---\n",
        "\n",
        "CONDITIONS = [\"neutral\", \"soft_smile\", \"big_smile\"]\n",
        "\n",
        "PROMPTS = {\n",
        "    \"neutral\":   \"portrait photo of a person, neutral facial expression, studio lighting\",\n",
        "    \"soft_smile\": \"portrait photo of a person, subtle soft smile, studio lighting\",\n",
        "    \"big_smile\":  \"portrait photo of a person, big bright smile, teeth visible, studio lighting\",\n",
        "}\n",
        "\n",
        "# Seeds we want to use\n",
        "SEEDS = [123, 124, 125, 126, 127]\n",
        "\n",
        "# SD generation hyperparameters\n",
        "HEIGHT = 768\n",
        "WIDTH = 768\n",
        "NUM_STEPS = 18\n",
        "GUIDANCE_SCALE = 5.0  # fixed here\n"
      ],
      "metadata": {
        "id": "G--uWhFROezO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CLIP model (text + image encoder) ---\n",
        "\n",
        "try:\n",
        "    clip_model\n",
        "    clip_processor\n",
        "    print(\"Reusing existing CLIP model.\")\n",
        "except NameError:\n",
        "    clip_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    clip_model_name = \"openai/clip-vit-large-patch14\"\n",
        "\n",
        "    clip_model = CLIPModel.from_pretrained(clip_model_name).to(clip_device)\n",
        "    clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "    clip_model.eval()\n",
        "    print(\"Loaded CLIP model on:\", clip_device)\n",
        "\n",
        "# --- LPIPS model ---\n",
        "\n",
        "try:\n",
        "    lpips_model\n",
        "    print(\"Reusing existing LPIPS model.\")\n",
        "except NameError:\n",
        "    lpips_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    lpips_model = lpips.LPIPS(net='vgg').to(lpips_device)\n",
        "    lpips_model.eval()\n",
        "    print(\"Loaded LPIPS model on:\", lpips_device)\n"
      ],
      "metadata": {
        "id": "slWHBYHvOoxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Basic image helpers ----------\n",
        "\n",
        "def load_image(path, size=None):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    if size is not None:\n",
        "        img = img.resize(size, Image.BICUBIC)\n",
        "    return img\n",
        "\n",
        "def img_to_numpy(img):\n",
        "    return np.asarray(img).astype(np.float32) / 255.0\n",
        "\n",
        "# Make sure lpips_device exists\n",
        "try:\n",
        "    lpips_device\n",
        "except NameError:\n",
        "    lpips_device = next(lpips_model.parameters()).device\n",
        "\n",
        "def img_to_lpips_tensor(img):\n",
        "    arr = np.asarray(img).astype(np.float32) / 255.0\n",
        "    arr = (arr * 2.0) - 1.0       # [-1, 1]\n",
        "    arr = torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(0)\n",
        "    return arr.to(lpips_device)\n",
        "# ---------- CLIP helpers ----------\n",
        "\n",
        "def get_clip_image_embeddings(images_pil):\n",
        "    \"\"\"Return [N, D] tensor of normalized CLIP image embeddings.\"\"\"\n",
        "    batch_size = 4\n",
        "    feats = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(images_pil), batch_size):\n",
        "            batch = images_pil[i:i+batch_size]\n",
        "            inputs = clip_processor(images=batch, return_tensors=\"pt\").to(clip_model.device)\n",
        "            out = clip_model.get_image_features(**inputs)\n",
        "            out = F.normalize(out, p=2, dim=-1)\n",
        "            feats.append(out)\n",
        "    return torch.cat(feats, dim=0)   # [N, D]\n",
        "\n",
        "def get_clip_text_embedding(text: str):\n",
        "    \"\"\"Return [D] normalized CLIP text embedding for a single string.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(clip_model.device)\n",
        "        text_feats = clip_model.get_text_features(**inputs)\n",
        "        text_feats = F.normalize(text_feats, p=2, dim=-1)\n",
        "    return text_feats[0]  # [D]\n",
        "\n",
        "# Optional: anchor phrases\n",
        "ANCHOR_TEXTS = [\n",
        "    \"neutral face portrait\",\n",
        "    \"smiling face portrait\",\n",
        "    \"serious face portrait\",\n",
        "    \"face looking confident\",\n",
        "    \"face looking surprised\",\n",
        "]\n",
        "\n",
        "def get_clip_anchor_embeddings(anchor_texts=ANCHOR_TEXTS):\n",
        "    with torch.no_grad():\n",
        "        inputs = clip_processor(text=anchor_texts, return_tensors=\"pt\", padding=True).to(clip_model.device)\n",
        "        feats = clip_model.get_text_features(**inputs)\n",
        "        feats = F.normalize(feats, p=2, dim=-1)\n",
        "    return feats   # [A, D]\n",
        "\n",
        "anchor_embs = get_clip_anchor_embeddings(ANCHOR_TEXTS)\n",
        "print(\"Anchor embeddings shape:\", anchor_embs.shape)\n"
      ],
      "metadata": {
        "id": "yTMbcApdO1Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_condition_paths(condition: str):\n",
        "    img_dir = os.path.join(IMG_ROOT, condition)\n",
        "    os.makedirs(img_dir, exist_ok=True)\n",
        "    return img_dir\n",
        "\n",
        "def expected_filenames_for_condition(condition: str, seeds):\n",
        "    return [f\"{condition}_seed{seed}.png\" for seed in seeds]\n",
        "\n",
        "def images_exist_for_condition(condition: str, seeds):\n",
        "    img_dir = get_condition_paths(condition)\n",
        "    expected = expected_filenames_for_condition(condition, seeds)\n",
        "    return all(os.path.exists(os.path.join(img_dir, fn)) for fn in expected)\n",
        "\n",
        "def generate_faces_for_condition(condition: str,\n",
        "                                prompt: str,\n",
        "                                seeds,\n",
        "                                pipe,\n",
        "                                height=HEIGHT,\n",
        "                                width=WIDTH,\n",
        "                                num_inference_steps=NUM_STEPS,\n",
        "                                guidance_scale=GUIDANCE_SCALE):\n",
        "    \"\"\"\n",
        "    Generate one image per seed for a given condition and prompt.\n",
        "    Skips generation if all expected files are already present.\n",
        "    \"\"\"\n",
        "    img_dir = get_condition_paths(condition)\n",
        "    expected_files = expected_filenames_for_condition(condition, seeds)\n",
        "\n",
        "    # Check if everything already exists\n",
        "    if all(os.path.exists(os.path.join(img_dir, fn)) for fn in expected_files):\n",
        "        print(f\"[{condition}] All {len(seeds)} images already exist. Skipping generation.\")\n",
        "        return img_dir\n",
        "\n",
        "    for seed in seeds:\n",
        "        fname = f\"{condition}_seed{seed}.png\"\n",
        "        out_path = os.path.join(img_dir, fname)\n",
        "        if os.path.exists(out_path):\n",
        "            print(f\"[{condition}] Seed {seed}: already exists, skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"[{condition}] Generating seed {seed}...\")\n",
        "        gen = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "\n",
        "        image = pipe(\n",
        "            prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            height=height,\n",
        "            width=width,\n",
        "            generator=gen,\n",
        "        ).images[0]\n",
        "\n",
        "        image.save(out_path)\n",
        "        print(f\"   saved {out_path}\")\n",
        "\n",
        "    return img_dir\n"
      ],
      "metadata": {
        "id": "7Kf-z5psO7OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_seed_diversity_for_condition(condition: str,\n",
        "                                         img_dir: str,\n",
        "                                         log_root: str = LOG_ROOT):\n",
        "    \"\"\"\n",
        "    - Loads all images for this condition.\n",
        "    - Computes pairwise LPIPS, SSIM, MSE.\n",
        "    - Saves:\n",
        "      - seed_diversity_pairwise_faces_<condition>.csv\n",
        "      - seed_diversity_summary_faces_<condition>.csv\n",
        "    \"\"\"\n",
        "    files = sorted([f for f in os.listdir(img_dir) if f.endswith(\".png\")])\n",
        "    if len(files) < 2:\n",
        "        print(f\"[{condition}] Not enough images for pairwise metrics.\")\n",
        "        return None, None\n",
        "\n",
        "    paths = [os.path.join(img_dir, f) for f in files]\n",
        "    seeds = [int(f.split(\"seed\")[-1].split(\".\")[0]) for f in files]\n",
        "\n",
        "    # Load images\n",
        "    images_np = []\n",
        "    images_lpips_t = []\n",
        "    target_size = None\n",
        "\n",
        "    for path in paths:\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if target_size is None:\n",
        "            target_size = img.size\n",
        "        img = img.resize(target_size, Image.BICUBIC)\n",
        "        images_np.append(img_to_numpy(img))\n",
        "        images_lpips_t.append(img_to_lpips_tensor(img))\n",
        "\n",
        "    print(f\"[{condition}] Loaded {len(images_np)} images of size {target_size}\")\n",
        "\n",
        "    # Pairwise metrics\n",
        "    results = []\n",
        "    n = len(images_np)\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            seed_i, seed_j = seeds[i], seeds[j]\n",
        "            path_i, path_j = paths[i], paths[j]\n",
        "\n",
        "            arr_i, arr_j = images_np[i], images_np[j]\n",
        "\n",
        "            mse_val = float(np.mean((arr_i - arr_j)**2))\n",
        "            ssim_val = float(ssim(arr_i, arr_j, channel_axis=-1, data_range=1.0))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                lpips_val = float(lpips_model(images_lpips_t[i], images_lpips_t[j]).item())\n",
        "\n",
        "            results.append({\n",
        "                \"condition\": condition,\n",
        "                \"seed_i\": seed_i,\n",
        "                \"seed_j\": seed_j,\n",
        "                \"image_i\": path_i,\n",
        "                \"image_j\": path_j,\n",
        "                \"MSE\": mse_val,\n",
        "                \"SSIM\": ssim_val,\n",
        "                \"LPIPS\": lpips_val,\n",
        "            })\n",
        "\n",
        "    pairwise_df = pd.DataFrame(results)\n",
        "    pairwise_path = os.path.join(log_root, f\"seed_diversity_pairwise_faces_{condition}.csv\")\n",
        "    pairwise_df.to_csv(pairwise_path, index=False)\n",
        "    print(f\"[{condition}] ✅ Saved pairwise seed diversity to:\", pairwise_path)\n",
        "\n",
        "    # Summary\n",
        "    summary = {\n",
        "        \"condition\": condition,\n",
        "        \"N_pairs\": len(pairwise_df),\n",
        "        \"LPIPS_mean\": pairwise_df[\"LPIPS\"].mean(),\n",
        "        \"LPIPS_std\":  pairwise_df[\"LPIPS\"].std(),\n",
        "        \"SSIM_mean\":  pairwise_df[\"SSIM\"].mean(),\n",
        "        \"SSIM_std\":   pairwise_df[\"SSIM\"].std(),\n",
        "        \"MSE_mean\":   pairwise_df[\"MSE\"].mean(),\n",
        "        \"MSE_std\":    pairwise_df[\"MSE\"].std(),\n",
        "    }\n",
        "    summary_df = pd.DataFrame([summary])\n",
        "    summary_path = os.path.join(log_root, f\"seed_diversity_summary_faces_{condition}.csv\")\n",
        "    summary_df.to_csv(summary_path, index=False)\n",
        "    print(f\"[{condition}] ✅ Saved seed diversity summary to:\", summary_path)\n",
        "\n",
        "    return pairwise_df, summary_df\n"
      ],
      "metadata": {
        "id": "V6pDX4_tO9Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_clip_diversity_for_condition(condition: str,\n",
        "                                         prompt: str,\n",
        "                                         img_dir: str,\n",
        "                                         log_root: str = LOG_ROOT):\n",
        "    \"\"\"\n",
        "    - Loads images for this condition.\n",
        "    - Computes CLIP:\n",
        "        - text–image similarity (prompt vs each image)\n",
        "        - image–image cosine sims / distances\n",
        "        - optional anchor similarities\n",
        "    - Also attaches LPIPS + SSIM for pairwise.\n",
        "    - Saves:\n",
        "        - clip_diversity_faces_<condition>.csv           (per-image)\n",
        "        - clip_diversity_pairwise_faces_<condition>.csv  (per-pair)\n",
        "    \"\"\"\n",
        "    files = sorted([f for f in os.listdir(img_dir) if f.endswith(\".png\")])\n",
        "    if len(files) < 2:\n",
        "        print(f\"[{condition}] Not enough images for CLIP pairwise metrics.\")\n",
        "        return None, None\n",
        "\n",
        "    paths = [os.path.join(img_dir, f) for f in files]\n",
        "    seeds = [int(f.split(\"seed\")[-1].split(\".\")[0]) for f in files]\n",
        "\n",
        "    # Load images\n",
        "    images_pil = []\n",
        "    images_np = []\n",
        "    images_lpips_t = []\n",
        "    target_size = None\n",
        "\n",
        "    for path in paths:\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if target_size is None:\n",
        "            target_size = img.size\n",
        "        img = img.resize(target_size, Image.BICUBIC)\n",
        "        images_pil.append(img)\n",
        "        images_np.append(img_to_numpy(img))\n",
        "        images_lpips_t.append(img_to_lpips_tensor(img))\n",
        "\n",
        "    print(f\"[{condition}] Loaded {len(images_pil)} images for CLIP metrics.\")\n",
        "\n",
        "    # Text embedding for the prompt\n",
        "    text_emb = get_clip_text_embedding(prompt)  # [D]\n",
        "\n",
        "    # Image embeddings\n",
        "    img_embs = get_clip_image_embeddings(images_pil)  # [N, D]\n",
        "\n",
        "    # --- Per-image text–image + anchor similarities ---\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # text–image cosine similarity\n",
        "        # img_embs: [N,D], text_emb: [D] -> [N]\n",
        "        text_sims = (img_embs @ text_emb)  # already normalized, so dot = cosine\n",
        "\n",
        "        # anchor sims: img_embs [N,D], anchor_embs [A,D] -> [N,A]\n",
        "        anchor_sims = img_embs @ anchor_embs.T\n",
        "\n",
        "    per_image_rows = []\n",
        "    anchor_col_names = [\n",
        "        f\"sim_anchor_{i}_\" + txt.replace(\" \", \"_\").replace(\",\", \"\").replace(\"/\", \"_\")\n",
        "        for i, txt in enumerate(ANCHOR_TEXTS)\n",
        "    ]\n",
        "\n",
        "    for idx, seed in enumerate(seeds):\n",
        "        row = {\n",
        "            \"condition\": condition,\n",
        "            \"seed\": seed,\n",
        "            \"image_path\": paths[idx],\n",
        "            \"CLIP_text_image_sim\": float(text_sims[idx].item()),\n",
        "        }\n",
        "        for name, val in zip(anchor_col_names, anchor_sims[idx].cpu().numpy().tolist()):\n",
        "            row[name] = val\n",
        "        per_image_rows.append(row)\n",
        "\n",
        "    per_image_df = pd.DataFrame(per_image_rows)\n",
        "    per_image_path = os.path.join(log_root, f\"clip_diversity_faces_{condition}.csv\")\n",
        "    per_image_df.to_csv(per_image_path, index=False)\n",
        "    print(f\"[{condition}] ✅ Saved per-image CLIP diversity to:\", per_image_path)\n",
        "\n",
        "    # --- Pairwise image–image CLIP + LPIPS + SSIM ---\n",
        "\n",
        "    pair_results = []\n",
        "    N = len(images_np)\n",
        "\n",
        "    for i in range(N):\n",
        "        for j in range(i+1, N):\n",
        "            seed_i, seed_j = seeds[i], seeds[j]\n",
        "            path_i, path_j = paths[i], paths[j]\n",
        "\n",
        "            arr_i, arr_j = images_np[i], images_np[j]\n",
        "\n",
        "            mse_val = float(np.mean((arr_i - arr_j)**2))\n",
        "            ssim_val = float(ssim(arr_i, arr_j, channel_axis=-1, data_range=1.0))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                lpips_val = float(lpips_model(images_lpips_t[i], images_lpips_t[j]).item())\n",
        "\n",
        "            v_i, v_j = img_embs[i], img_embs[j]\n",
        "            clip_sim = float((v_i * v_j).sum().item())\n",
        "            clip_dist = 1.0 - clip_sim\n",
        "\n",
        "            pair_results.append({\n",
        "                \"condition\": condition,\n",
        "                \"seed_i\": seed_i,\n",
        "                \"seed_j\": seed_j,\n",
        "                \"image_i\": path_i,\n",
        "                \"image_j\": path_j,\n",
        "                \"MSE\": mse_val,\n",
        "                \"SSIM\": ssim_val,\n",
        "                \"LPIPS\": lpips_val,\n",
        "                \"CLIP_cosine_sim\": clip_sim,\n",
        "                \"CLIP_cosine_dist\": clip_dist,\n",
        "            })\n",
        "\n",
        "    pairwise_df = pd.DataFrame(pair_results)\n",
        "    pairwise_path = os.path.join(log_root, f\"clip_diversity_pairwise_faces_{condition}.csv\")\n",
        "    pairwise_df.to_csv(pairwise_path, index=False)\n",
        "    print(f\"[{condition}] ✅ Saved pairwise CLIP diversity to:\", pairwise_path)\n",
        "\n",
        "    return per_image_df, pairwise_df\n"
      ],
      "metadata": {
        "id": "gFlGdH5ZO_Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_pairwise_seed   = {}\n",
        "all_summary_seed    = {}\n",
        "all_clip_pairwise   = {}\n",
        "all_clip_per_image  = {}\n",
        "\n",
        "for cond in CONDITIONS:\n",
        "    prompt = PROMPTS[cond]\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"CONDITION: {cond}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1) Generate (skip if exists)\n",
        "    img_dir = generate_faces_for_condition(cond, prompt, SEEDS, pipe)\n",
        "\n",
        "    # 2) Seed diversity metrics (LPIPS/SSIM/MSE)\n",
        "    pair_df_seed, summary_df_seed = compute_seed_diversity_for_condition(cond, img_dir)\n",
        "    all_pairwise_seed[cond] = pair_df_seed\n",
        "    all_summary_seed[cond] = summary_df_seed\n",
        "\n",
        "    # 3) CLIP diversity metrics (text–image + image–image)\n",
        "    clip_per_image_df, clip_pairwise_df = compute_clip_diversity_for_condition(cond, prompt, img_dir)\n",
        "    all_clip_per_image[cond] = clip_per_image_df\n",
        "    all_clip_pairwise[cond] = clip_pairwise_df\n"
      ],
      "metadata": {
        "id": "IZiBu2qsPBlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_face_diversity_summary(log_root: str = LOG_ROOT, conditions=CONDITIONS):\n",
        "    # Load all pairwise CLIP files into one DF\n",
        "    dfs = []\n",
        "    for cond in conditions:\n",
        "        path = os.path.join(log_root, f\"clip_diversity_pairwise_faces_{cond}.csv\")\n",
        "        if os.path.exists(path):\n",
        "            df = pd.read_csv(path)\n",
        "            df[\"condition\"] = cond\n",
        "            dfs.append(df)\n",
        "        else:\n",
        "            print(f\"Missing pairwise CLIP file for {cond}: {path}\")\n",
        "    if not dfs:\n",
        "        print(\"No CLIP pairwise data to plot.\")\n",
        "        return\n",
        "\n",
        "    pairwise_all = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # 1) Histograms of CLIP distance\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.histplot(data=pairwise_all, x=\"CLIP_cosine_dist\", hue=\"condition\", bins=15, kde=True, alpha=0.5)\n",
        "    plt.title(\"CLIP cosine distance distribution by condition\")\n",
        "    plt.xlabel(\"CLIP cosine distance (1 - cosine similarity)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2) Boxplots: LPIPS, SSIM, CLIP distance\n",
        "    plt.figure(figsize=(12,4))\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    sns.boxplot(data=pairwise_all, x=\"condition\", y=\"LPIPS\")\n",
        "    plt.title(\"LPIPS by condition\")\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    sns.boxplot(data=pairwise_all, x=\"condition\", y=\"SSIM\")\n",
        "    plt.title(\"SSIM by condition\")\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    sns.boxplot(data=pairwise_all, x=\"condition\", y=\"CLIP_cosine_dist\")\n",
        "    plt.title(\"CLIP distance by condition\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 3) Scatter LPIPS vs CLIP distance\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.scatterplot(data=pairwise_all, x=\"CLIP_cosine_dist\", y=\"LPIPS\", hue=\"condition\")\n",
        "    plt.xlabel(\"CLIP cosine distance\")\n",
        "    plt.ylabel(\"LPIPS\")\n",
        "    plt.title(\"Perceptual vs semantic distance\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_face_diversity_summary(LOG_ROOT, CONDITIONS)\n"
      ],
      "metadata": {
        "id": "XX2PqcriQbkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_face_diversity_text_summary(log_root: str = LOG_ROOT, conditions=CONDITIONS):\n",
        "    print(\"\\n===== NUMERIC SUMMARY (mean ± std) =====\\n\")\n",
        "\n",
        "    # We can reuse the seed_diversity_summary_faces_* and clip_diversity_pairwise_faces_*\n",
        "    for cond in conditions:\n",
        "        # Seed diversity summary\n",
        "        seed_summary_path = os.path.join(log_root, f\"seed_diversity_summary_faces_{cond}.csv\")\n",
        "        clip_pairwise_path = os.path.join(log_root, f\"clip_diversity_pairwise_faces_{cond}.csv\")\n",
        "\n",
        "        if not os.path.exists(seed_summary_path) or not os.path.exists(clip_pairwise_path):\n",
        "            print(f\"[{cond}] Missing summary files.\")\n",
        "            continue\n",
        "\n",
        "        seed_summary = pd.read_csv(seed_summary_path).iloc[0]\n",
        "        clip_pairwise = pd.read_csv(clip_pairwise_path)\n",
        "\n",
        "        lpips_mean, lpips_std = seed_summary[\"LPIPS_mean\"], seed_summary[\"LPIPS_std\"]\n",
        "        ssim_mean,  ssim_std  = seed_summary[\"SSIM_mean\"],  seed_summary[\"SSIM_std\"]\n",
        "        mse_mean,   mse_std   = seed_summary[\"MSE_mean\"],   seed_summary[\"MSE_std\"]\n",
        "\n",
        "        clip_dist_mean = clip_pairwise[\"CLIP_cosine_dist\"].mean()\n",
        "        clip_dist_std  = clip_pairwise[\"CLIP_cosine_dist\"].std()\n",
        "\n",
        "        print(f\"Condition: {cond}\")\n",
        "        print(f\"  LPIPS: {lpips_mean:.4f} ± {lpips_std:.4f}\")\n",
        "        print(f\"  SSIM:  {ssim_mean:.4f} ± {ssim_std:.4f}\")\n",
        "        print(f\"  MSE:   {mse_mean:.6f} ± {mse_std:.6f}\")\n",
        "        print(f\"  CLIP distance: {clip_dist_mean:.4f} ± {clip_dist_std:.4f}\")\n",
        "        print()\n",
        "\n",
        "    print(\"# Interpretation guidelines :\")\n",
        "    print(\"# - Higher LPIPS & CLIP distance   → more perceptual & semantic diversity.\")\n",
        "    print(\"# - Lower SSIM & higher MSE        → more structural changes.\")\n",
        "    print(\"# - Compare neutral vs soft_smile vs big_smile to argue how increasing smile intensity\")\n",
        "    print(\"#   changes semantic diversity (CLIP) vs low-level/perceptual diversity (LPIPS/MSE).\")\n",
        "\n",
        "print_face_diversity_text_summary(LOG_ROOT, CONDITIONS)\n"
      ],
      "metadata": {
        "id": "Wd8jMNhSQyc3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}