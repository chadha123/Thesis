{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "collapsed_sections": [
        "3QnoX9uWSev0",
        "c0WwN7WnTAa2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# initial training"
      ],
      "metadata": {
        "id": "3QnoX9uWSev0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CORE IMPORTS & SETUP ====\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CelebA\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ---- PATHS ----\n",
        "ROOT_DIR = \"/content\"\n",
        "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_DIR = \"/content/drive/MyDrive/thesis2/models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "I6hlEEg2SNwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BEST_MODEL_PATH = os.path.join(MODEL_DIR, \"smile_classifier_best.pt\")\n",
        "LOG_PATH        = os.path.join(MODEL_DIR, \"smile_classifier_training_log.csv\")\n",
        "\n",
        "print(\"Best model will be saved to:\", BEST_MODEL_PATH)\n",
        "print(\"Training log will be saved to:\", LOG_PATH)\n",
        "\n",
        "# ---- DEVICE ----\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "2VuV_9bKR_nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16lbRnteg44n"
      },
      "outputs": [],
      "source": [
        "# ---- TRAINING CONFIG (compute-friendly) ----\n",
        "MAX_TRAIN_SAMPLES = 25_000   # subset of CelebA for speed\n",
        "VAL_RATIO = 0.1\n",
        "\n",
        "BATCH_SIZE    = 128\n",
        "MAX_EPOCHS    = 15      # upper bound, early stopping will cut earlier\n",
        "PATIENCE      = 3       # epochs without val improvement before stopping\n",
        "LEARNING_RATE = 3e-4\n",
        "USE_AMP       = True    # mixed precision on L4 GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CELEBA WRAPPER FOR SMILE LABEL ====\n",
        "class CelebASmileDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps torchvision.datasets.CelebA to provide:\n",
        "      - image (PIL -> transformed tensor)\n",
        "      - label: 0 = not smiling, 1 = smiling\n",
        "    \"\"\"\n",
        "    def __init__(self, root, split=\"train\", transform=None, download=False):\n",
        "        self.celeba = CelebA(\n",
        "            root=root,\n",
        "            split=split,\n",
        "            target_type=\"attr\",\n",
        "            download=download,\n",
        "        )\n",
        "        self.transform = transform\n",
        "\n",
        "        # Find index of \"Smiling\" attribute\n",
        "        self.smile_idx = self.celeba.attr_names.index(\"Smiling\")\n",
        "        print(f\"Split={split}: total samples = {len(self.celeba)}\")\n",
        "        print(\"Smiling attribute index:\", self.smile_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.celeba)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, attrs = self.celeba[idx]\n",
        "        # attrs is a tensor of shape [40], values -1 or 1\n",
        "        smile_attr = attrs[self.smile_idx].item()\n",
        "        label = 1 if smile_attr == 1 else 0  # smiling -> 1, else 0\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n"
      ],
      "metadata": {
        "id": "R1Lstr2nh5Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== TRANSFORMS ====\n",
        "# Stronger augmentation on train to reduce overfitting\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std =[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Validation: no augmentation, just resize + normalize\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std =[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "# ==== LOAD CELEBA TRAIN SPLIT ====\n",
        "full_train_ds = CelebASmileDataset(\n",
        "    root=DATA_DIR,\n",
        "    split=\"train\",\n",
        "    transform=train_transform,\n",
        "    download=True,\n",
        ")\n",
        "\n",
        "# Subsample for compute reasons\n",
        "if MAX_TRAIN_SAMPLES is not None and len(full_train_ds) > MAX_TRAIN_SAMPLES:\n",
        "    full_train_ds, _ = random_split(\n",
        "        full_train_ds,\n",
        "        [MAX_TRAIN_SAMPLES, len(full_train_ds) - MAX_TRAIN_SAMPLES]\n",
        "    )\n",
        "    print(f\"Subsampled train dataset to {MAX_TRAIN_SAMPLES} samples.\")\n",
        "\n",
        "# Train / val split\n",
        "num_train = int((1.0 - VAL_RATIO) * len(full_train_ds))\n",
        "num_val   = len(full_train_ds) - num_train\n",
        "\n",
        "train_ds, val_ds = random_split(full_train_ds, [num_train, num_val])\n",
        "print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}\")\n",
        "\n",
        "# Set val transform explicitly\n",
        "val_ds.dataset.transform = val_transform\n",
        "\n",
        "# ==== DATALOADERS ====\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "InR4cHaCh7zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ==== MODEL CREATION ====\n",
        "def create_smile_model():\n",
        "    m = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "    num_features = m.fc.in_features\n",
        "    # Add dropout to combat overfitting\n",
        "    m.fc = nn.Sequential(\n",
        "        nn.Dropout(p=0.4),\n",
        "        nn.Linear(num_features, 2),\n",
        "    )\n",
        "    return m\n",
        "\n",
        "model = create_smile_model().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "scaler = GradScaler(enabled=USE_AMP)\n",
        "\n",
        "print(\"Model ready on:\", device)\n"
      ],
      "metadata": {
        "id": "WjQDoMIch-_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== TRAIN & EVAL HELPERS ====\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device, scaler=None, use_amp=True):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for imgs, labels in tqdm(dataloader, desc=\"Train\", leave=False):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if use_amp and scaler is not None:\n",
        "            with autocast():\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(dataloader, desc=\"Val\", leave=False):\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc\n"
      ],
      "metadata": {
        "id": "_1fvnMNeiAs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== TRAINING WITH EARLY STOPPING & LOGGING ====\n",
        "best_val_acc = 0.0\n",
        "best_epoch = 0\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "epochs_no_improve = 0\n",
        "\n",
        "history = {\n",
        "    \"epoch\": [],\n",
        "    \"train_loss\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_acc\": [],\n",
        "}\n",
        "\n",
        "print(f\"Starting training with early stopping (patience={PATIENCE})\")\n",
        "\n",
        "for epoch in range(1, MAX_EPOCHS + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{MAX_EPOCHS}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        device,\n",
        "        scaler=scaler,\n",
        "        use_amp=USE_AMP,\n",
        "    )\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}/{MAX_EPOCHS} \"\n",
        "          f\"- train loss: {train_loss:.4f} - train acc: {train_acc:.4f} \"\n",
        "          f\"- val loss: {val_loss:.4f} - val acc: {val_acc:.4f}\")\n",
        "\n",
        "    # log history\n",
        "    history[\"epoch\"].append(epoch)\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"train_acc\"].append(train_acc)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "    # check improvement\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_epoch = epoch\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        torch.save(best_model_wts, BEST_MODEL_PATH)\n",
        "        print(f\"  ‚úÖ New best model at epoch {epoch}, val acc = {best_val_acc:.4f}\")\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"  No val improvement for {epochs_no_improve} epoch(s).\")\n",
        "\n",
        "    # early stopping\n",
        "    if epochs_no_improve >= PATIENCE:\n",
        "        print(f\"\\n‚èπ Early stopping triggered at epoch {epoch}.\")\n",
        "        break\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "print(f\"Best epoch: {best_epoch} with val acc = {best_val_acc:.4f}\")\n",
        "print(\"Best model saved to:\", BEST_MODEL_PATH)\n",
        "\n",
        "# save history to CSV\n",
        "df_history = pd.DataFrame(history)\n",
        "df_history.to_csv(LOG_PATH, index=False)\n",
        "print(\"Training log saved to:\", LOG_PATH)\n"
      ],
      "metadata": {
        "id": "67iSXD_niDF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_curves(history):\n",
        "    epochs = history[\"epoch\"]\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\", marker=\"o\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training vs Validation Loss\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\", marker=\"o\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\", marker=\"o\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Training vs Validation Accuracy\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call this after training:\n",
        "plot_training_curves(history)\n"
      ],
      "metadata": {
        "id": "ZYwlXZvEiGO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# --- same create_smile_model as in training ---\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "import torch.nn as nn\n",
        "\n",
        "def create_smile_model():\n",
        "    m = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "    num_features = m.fc.in_features\n",
        "    m.fc = nn.Sequential(\n",
        "        nn.Dropout(p=0.4),\n",
        "        nn.Linear(num_features, 2),\n",
        "    )\n",
        "    return m\n",
        "\n",
        "# load best weights from Drive\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "smile_model = create_smile_model()\n",
        "smile_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
        "smile_model.to(device)\n",
        "smile_model.eval()\n",
        "\n",
        "print(\"Loaded best smile classifier from:\", BEST_MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "uaKJqpkWmHie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing for test images (same as val)\n",
        "inference_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std =[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    return inference_transform(img), img  # return tensor + original PIL for display\n",
        "\n",
        "def predict_smile_probability(model, image_tensor, device):\n",
        "    \"\"\"\n",
        "    image_tensor: (3, H, W) normalized as in training.\n",
        "    returns: float in [0,1] = probability of 'smiling' (class 1)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = image_tensor.unsqueeze(0).to(device)  # (1,3,H,W)\n",
        "        logits = model(x)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        smile_prob = probs[0, 1].item()\n",
        "    return smile_prob\n"
      ],
      "metadata": {
        "id": "B1lXTS90mLB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fname in uploaded.keys():\n",
        "    print(\"Testing:\", fname)\n",
        "    img_tensor, img_pil = load_and_preprocess_image(fname)\n",
        "    p_smile = predict_smile_probability(smile_model, img_tensor, device)\n",
        "\n",
        "    plt.imshow(img_pil)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"{fname}\\nP(smiling) = {p_smile:.3f}\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Smile probability:\", p_smile)\n",
        "    print(\"Prediction:\", \"SMILING üòÑ\" if p_smile >= 0.5 else \"NOT SMILING üòê\")\n"
      ],
      "metadata": {
        "id": "Fh1FNnUGmNKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fine tuning"
      ],
      "metadata": {
        "id": "c0WwN7WnTAa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 1: imports + paths + device =======================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        ")\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ---- Paths ----\n",
        "DATA_ROOT     = \"/content/drive/MyDrive/thesis2/classifier_dataset\"\n",
        "OLD_MODEL_PATH = \"/content/drive/MyDrive/thesis2/models/smile_classifier_best.pt\"\n",
        "NEW_MODEL_PATH = \"/content/drive/MyDrive/thesis2/models/smile_classifier_sd35_finetuned.pt\"\n",
        "PLOTS_DIR      = \"/content/drive/MyDrive/thesis2/plots\"\n",
        "\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Device ----\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "YZ3nSHwOTDai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 2: recreate model + load previous weights =========================\n",
        "\n",
        "def create_smile_model():\n",
        "    \"\"\"\n",
        "    ResNet18 backbone pretrained on ImageNet, with:\n",
        "    - dropout\n",
        "    - 2-class output (neutral, smiling)\n",
        "    \"\"\"\n",
        "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(p=0.4),\n",
        "        nn.Linear(num_features, 2),\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = create_smile_model().to(device)\n",
        "\n",
        "state_dict = torch.load(OLD_MODEL_PATH, map_location=device)\n",
        "load_result = model.load_state_dict(state_dict, strict=False)\n",
        "print(\"Loaded state dict from:\", OLD_MODEL_PATH)\n",
        "print(\"Missing keys   :\", load_result.missing_keys)\n",
        "print(\"Unexpected keys:\", load_result.unexpected_keys)\n",
        "\n",
        "if not load_result.missing_keys and not load_result.unexpected_keys:\n",
        "    print(\" State dict loaded cleanly.\")\n",
        "else:\n",
        "    print(\" There are missing/unexpected keys ‚Äì check if architecture changed.\")\n"
      ],
      "metadata": {
        "id": "7Ife_x7Pe_mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 3: dataset, transforms, dataloaders ===============================\n",
        "\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "IMG_SIZE = 512\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "# Base dataset just to know files + classes\n",
        "base_dataset = ImageFolder(root=DATA_ROOT)\n",
        "num_samples = len(base_dataset)\n",
        "print(\"Total samples:\", num_samples)\n",
        "print(\"Classes:\", base_dataset.classes)\n",
        "\n",
        "# 70 / 15 / 15 split\n",
        "n_train = int(0.70 * num_samples)\n",
        "n_val   = int(0.15 * num_samples)\n",
        "n_test  = num_samples - n_train - n_val\n",
        "\n",
        "g = torch.Generator().manual_seed(42)\n",
        "indices = torch.randperm(num_samples, generator=g).tolist()\n",
        "\n",
        "train_idx = indices[:n_train]\n",
        "val_idx   = indices[n_train:n_train + n_val]\n",
        "test_idx  = indices[n_train + n_val:]\n",
        "\n",
        "# Datasets with transforms\n",
        "train_dataset_full = ImageFolder(root=DATA_ROOT, transform=train_transform)\n",
        "val_dataset_full   = ImageFolder(root=DATA_ROOT, transform=val_test_transform)\n",
        "test_dataset_full  = ImageFolder(root=DATA_ROOT, transform=val_test_transform)\n",
        "\n",
        "train_dataset = Subset(train_dataset_full, train_idx)\n",
        "val_dataset   = Subset(val_dataset_full,   val_idx)\n",
        "test_dataset  = Subset(test_dataset_full,  test_idx)\n",
        "\n",
        "print(f\"Train / Val / Test sizes: {len(train_dataset)}, {len(val_dataset)}, {len(test_dataset)}\")\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64 if device.type == \"cuda\" else 32\n",
        "print(\"Using batch size:\", BATCH_SIZE)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=2, pin_memory=(device.type == \"cuda\"),\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=2, pin_memory=(device.type == \"cuda\"),\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=2, pin_memory=(device.type == \"cuda\"),\n",
        ")\n"
      ],
      "metadata": {
        "id": "sLrC_6i_fD_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 4: freeze backbone, set optimizer, scheduler, loss ===============\n",
        "\n",
        "# 1) Freeze everything\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# 2) Unfreeze layer3, layer4, and fc head\n",
        "for name, p in model.named_parameters():\n",
        "    if name.startswith(\"layer3\") or name.startswith(\"layer4\") or name.startswith(\"fc.\"):\n",
        "        p.requires_grad = True\n",
        "\n",
        "backbone_l3_params = []\n",
        "backbone_l4_params = []\n",
        "head_params        = []\n",
        "\n",
        "for name, p in model.named_parameters():\n",
        "    if not p.requires_grad:\n",
        "        continue\n",
        "    if name.startswith(\"layer3\"):\n",
        "        backbone_l3_params.append(p)\n",
        "    elif name.startswith(\"layer4\"):\n",
        "        backbone_l4_params.append(p)\n",
        "    elif name.startswith(\"fc.\"):\n",
        "        head_params.append(p)\n",
        "\n",
        "print(\n",
        "    f\"Trainable params ‚Äì layer3: {len(backbone_l3_params)}, \"\n",
        "    f\"layer4: {len(backbone_l4_params)}, head: {len(head_params)}\"\n",
        ")\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    [\n",
        "        {\"params\": backbone_l3_params, \"lr\": 5e-6},\n",
        "        {\"params\": backbone_l4_params, \"lr\": 1e-5},\n",
        "        {\"params\": head_params,        \"lr\": 5e-5},\n",
        "    ],\n",
        "    weight_decay=1e-4,\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wl1jHaTHfGqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LR scheduler ‚Äì reduces LR when val loss plateaus\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode=\"min\",\n",
        "    factor=0.3,\n",
        "    patience=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "Pap4B2QLmUa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 5: helper functions ==============================================\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc  = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc  = correct / total\n",
        "    return epoch_loss, epoch_acc\n"
      ],
      "metadata": {
        "id": "LwGJXOWyfJZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 6: training loop with early stopping + scheduler ==================\n",
        "\n",
        "MAX_EPOCHS = 50\n",
        "PATIENCE   = 6  # epochs without val-loss improvement\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accs,   val_accs   = [], []\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "best_epoch    = -1\n",
        "epochs_no_improve = 0\n",
        "\n",
        "for epoch in range(1, MAX_EPOCHS + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{MAX_EPOCHS}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss,   val_acc   = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(\n",
        "        f\"Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | \"\n",
        "        f\"Val loss: {val_loss:.4f} | Val acc: {val_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "    # Scheduler step on validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping logic\n",
        "    if val_loss < best_val_loss - 1e-4:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), NEW_MODEL_PATH)\n",
        "        print(f\"  New best model saved (epoch {epoch}) with val loss {val_loss:.4f}\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"  No improvement in val loss for {epochs_no_improve} epoch(s).\")\n",
        "\n",
        "    if epochs_no_improve >= PATIENCE:\n",
        "        print(\"\\n‚èπ Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "print(f\"Best epoch: {best_epoch}, best val loss: {best_val_loss:.4f}\")\n",
        "print(\"Best model path:\", NEW_MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "6TFjKQ02fLzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 7: save history + plot curves ====================================\n",
        "\n",
        "history = {\n",
        "    \"epoch\": list(range(1, len(train_losses) + 1)),\n",
        "    \"train_loss\": train_losses,\n",
        "    \"val_loss\":   val_losses,\n",
        "    \"train_acc\":  train_accs,\n",
        "    \"val_acc\":    val_accs,\n",
        "}\n",
        "\n",
        "log_csv_path  = os.path.join(PLOTS_DIR, \"sd35_finetune_training_log.csv\")\n",
        "log_json_path = os.path.join(PLOTS_DIR, \"sd35_finetune_training_log.json\")\n",
        "curves_path   = os.path.join(PLOTS_DIR, \"sd35_finetune_training_curves.png\")\n",
        "\n",
        "# Save CSV\n",
        "df_hist = pd.DataFrame(history)\n",
        "df_hist.to_csv(log_csv_path, index=False)\n",
        "print(\"Saved training log CSV to:\", log_csv_path)\n",
        "\n",
        "# Save JSON\n",
        "with open(log_json_path, \"w\") as f:\n",
        "    json.dump(history, f, indent=4)\n",
        "print(\"Saved training log JSON to:\", log_json_path)\n",
        "\n",
        "# Plot curves\n",
        "epochs = history[\"epoch\"]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(epochs, train_losses, label=\"Train Loss\")\n",
        "axes[0].plot(epochs, val_losses,   label=\"Val Loss\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].set_title(\"Training vs Validation Loss\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Accuracy\n",
        "axes[1].plot(epochs, train_accs, label=\"Train Acc\")\n",
        "axes[1].plot(epochs, val_accs,   label=\"Val Acc\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "axes[1].set_title(\"Training vs Validation Accuracy\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(curves_path, dpi=200)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved training curves to:\", curves_path)\n"
      ],
      "metadata": {
        "id": "veCYU9Nsie_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 8: test metrics + confusion matrix ===============================\n",
        "\n",
        "best_model = create_smile_model().to(device)\n",
        "best_model.load_state_dict(torch.load(NEW_MODEL_PATH, map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "all_labels = []\n",
        "all_preds  = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        outputs = best_model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "\n",
        "all_labels = np.concatenate(all_labels)\n",
        "all_preds  = np.concatenate(all_preds)\n",
        "\n",
        "test_acc = accuracy_score(all_labels, all_preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    all_labels, all_preds, average=\"binary\", pos_label=1\n",
        ")\n",
        "\n",
        "print(f\"Test accuracy        : {test_acc:.4f}\")\n",
        "print(f\"Precision (smiling)  : {precision:.4f}\")\n",
        "print(f\"Recall (smiling)     : {recall:.4f}\")\n",
        "print(f\"F1-score (smiling)   : {f1:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"\\nConfusion matrix (rows=true, cols=pred):\\n\", cm)\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm_path = os.path.join(PLOTS_DIR, \"smile_classifier_sd35_confusion_matrix.png\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4, 4))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "    xticklabels=base_dataset.classes,\n",
        "    yticklabels=base_dataset.classes,\n",
        "    ax=ax,\n",
        ")\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "ax.set_title(\"Smile vs Neutral ‚Äì Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(cm_path, dpi=200)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved confusion matrix plot to:\", cm_path)\n"
      ],
      "metadata": {
        "id": "Gtw1crpPfzF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 9: predict on a single image =====================================\n",
        "\n",
        "idx_to_class = {v: k for k, v in base_dataset.class_to_idx.items()}\n",
        "print(\"Class mapping:\", idx_to_class)\n",
        "\n",
        "def predict_image(model, image_path, device):\n",
        "    \"\"\"\n",
        "    Run the fine-tuned model on a single image path.\n",
        "    Prints predicted label and P(smiling).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    x = val_test_transform(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "    pred_idx = int(np.argmax(probs))\n",
        "    pred_label = idx_to_class[pred_idx]\n",
        "    prob_smile = float(probs[base_dataset.class_to_idx[\"smiling\"]])\n",
        "\n",
        "    print(f\"Image: {image_path}\")\n",
        "    print(f\"Predicted label : {pred_label}\")\n",
        "    print(f\"P(smiling)      : {prob_smile:.4f}\")\n",
        "    return pred_label, prob_smile\n",
        "\n",
        "# --- TEST A FEW SAMPLE IMAGES ---\n",
        "\n",
        "sample_paths = [\n",
        "    \"/content/drive/MyDrive/thesis2/classifier_dataset/neutral/neutral_0000_p1.png\",\n",
        "    \"/content/drive/MyDrive/thesis2/classifier_dataset/smiling/smiling_0001_p2.png\",\n",
        "]\n",
        "\n",
        "for path in sample_paths:\n",
        "    print(\"\\nTesting:\", path)\n",
        "    predict_image(best_model, path, device)\n",
        "\n"
      ],
      "metadata": {
        "id": "2e0Ls7DBf_tY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}