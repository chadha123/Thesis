{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eM-Ma6Q2Syjh"
      ],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# counterfactual generation with CLIP sampling"
      ],
      "metadata": {
        "id": "eM-Ma6Q2Syjh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7hhuETDjs6J"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from diffusers import StableDiffusion3Pipeline\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q diffusers==0.36.0 transformers accelerate lpips"
      ],
      "metadata": {
        "id": "wjJiBYKlQwyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 0. Imports + basic setup\n",
        "# ============================================================\n",
        "import os, glob, random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from diffusers import StableDiffusion3Img2ImgPipeline\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "ZwYwzEu1Qi2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1. Paths & global config\n",
        "# ============================================================\n",
        "BASE_DIR        = \"/content/drive/MyDrive/thesis2\"\n",
        "NEUTRAL_DIR     = f\"{BASE_DIR}/classifier_dataset/neutral\"\n",
        "CF_OUT_DIR      = f\"{BASE_DIR}/phase4_clip_sampling_images\"\n",
        "CSV_CF_PATH     = f\"{BASE_DIR}/phase4_clip_sampling_metadata.csv\"\n",
        "\n",
        "os.makedirs(CF_OUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "mlrMDIxF9Nl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# SD settings\n",
        "HEIGHT  = 768\n",
        "WIDTH   = 768\n",
        "NUM_STEPS = 30\n",
        "GUIDANCE = 7.0\n",
        "STRENGTH = 0.55         # how strong the transformation is in img2img\n",
        "\n",
        "N_SOURCE = 5            # how many neutral sources\n",
        "N_RAW_CF = 12           # how many raw CF per source before CLIP selection\n",
        "N_KEEP_CF = 6           # how many we keep after CLIP scoring\n",
        "\n",
        "BASE_SEED = 12345\n",
        "\n",
        "# ============================================================\n",
        "# 2. Load Stable Diffusion 3.5 **img2img** pipeline\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "model_id = \"stabilityai/stable-diffusion-3.5-medium\"\n",
        "\n",
        "pipe = StableDiffusion3Img2ImgPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    token=HF_TOKEN,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "# light memory optimizations\n",
        "pipe.enable_attention_slicing()\n",
        "pipe.set_progress_bar_config(disable=True)\n",
        "\n",
        "print(\"SD3.5 Img2Img pipeline loaded.\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. Load CLIP model (for semantic scoring)\n",
        "# ============================================================\n",
        "clip_model_id = \"openai/clip-vit-base-patch32\"\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_id).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_id)\n",
        "clip_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def clip_text_embeddings(texts):\n",
        "    \"\"\"Return L2-normalized CLIP text embeddings for a list of strings.\"\"\"\n",
        "    inputs = clip_processor(text=texts, images=None, return_tensors=\"pt\", padding=True).to(device)\n",
        "    text_feats = clip_model.get_text_features(**inputs)\n",
        "    text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
        "    return text_feats\n",
        "\n",
        "neutral_text = \"portrait photo of a person with a neutral facial expression\"\n",
        "smile_text   = \"portrait photo of a person with a big visible smile, teeth showing, joyful expression\"\n",
        "\n",
        "txt_embs = clip_text_embeddings([neutral_text, smile_text])\n",
        "txt_neutral_emb = txt_embs[0:1]\n",
        "txt_smile_emb   = txt_embs[1:2]\n",
        "\n",
        "@torch.no_grad()\n",
        "def clip_smile_score(img: Image.Image) -> float:\n",
        "    \"\"\"\n",
        "    CLIP-based 'smile score':\n",
        "      score = sim(image, smile_text) - sim(image, neutral_text)\n",
        "    Higher score => more smiley.\n",
        "    \"\"\"\n",
        "    inputs = clip_processor(text=None, images=img, return_tensors=\"pt\").to(device)\n",
        "    img_feats = clip_model.get_image_features(**inputs)\n",
        "    img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    sim_smile   = (img_feats @ txt_smile_emb.T).item()\n",
        "    sim_neutral = (img_feats @ txt_neutral_emb.T).item()\n",
        "    return float(sim_smile - sim_neutral)\n",
        "\n",
        "print(\"CLIP model loaded.\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. Prompts\n",
        "# ============================================================\n",
        "BASE_FACE_PROMPT = (\n",
        "    \"a photorealistic portrait of a human face, studio lighting, high resolution, \"\n",
        "    \"natural skin texture, realistic anatomy, professional photography, symmetric face, looking forward\"\n",
        ")\n",
        "\n",
        "NEUTRAL_PROMPT = (\n",
        "    BASE_FACE_PROMPT +\n",
        "    \", neutral facial expression, relaxed mouth, closed lips, no smile\"\n",
        ")\n",
        "\n",
        "SMILE_PROMPT = (\n",
        "    BASE_FACE_PROMPT +\n",
        "    \", big, bright smile, teeth clearly visible, joyful and expressive, cheeks raised, eyes slightly squinting\"\n",
        ")\n",
        "\n",
        "print(\"Neutral prompt:\\n \", NEUTRAL_PROMPT)\n",
        "print(\"Smile prompt:\\n \", SMILE_PROMPT)\n",
        "\n",
        "# ============================================================\n",
        "# 5. Build df_sources from neutral dataset (existing SD images)\n",
        "# ============================================================\n",
        "neutral_paths = sorted(\n",
        "    glob.glob(os.path.join(NEUTRAL_DIR, \"*.png\")) +\n",
        "    glob.glob(os.path.join(NEUTRAL_DIR, \"*.jpg\")) +\n",
        "    glob.glob(os.path.join(NEUTRAL_DIR, \"*.jpeg\"))\n",
        ")\n",
        "\n",
        "if len(neutral_paths) == 0:\n",
        "    raise RuntimeError(f\"No images found in {NEUTRAL_DIR}\")\n",
        "\n",
        "neutral_paths = neutral_paths[:N_SOURCE]\n",
        "\n",
        "import glob, os\n",
        "import pandas as pd\n",
        "\n",
        "CUSTOM_SOURCE_DIR = \"/content/drive/MyDrive/thesis2/custom_sources\"\n",
        "\n",
        "source_paths = sorted(glob.glob(os.path.join(CUSTOM_SOURCE_DIR, \"*\")))\n",
        "print(\"Found\", len(source_paths), \"source images\")\n",
        "\n",
        "df_sources = pd.DataFrame({\n",
        "    \"source_id\": list(range(len(source_paths))),\n",
        "    \"source_path\": source_paths\n",
        "})\n",
        "\n",
        "df_sources.head()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6. Helper: generate CLIP-sampled smiling CFs for ONE source\n",
        "# ============================================================\n",
        "@dataclass\n",
        "class CFEntry:\n",
        "    source_id: int\n",
        "    cf_rank: int\n",
        "    raw_idx: int\n",
        "    seed: int\n",
        "    clip_score: float\n",
        "    cf_path: str\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_cfs_for_source(source_id: int, source_path: str) -> list:\n",
        "    \"\"\"\n",
        "    For a single neutral source image, generate N_RAW_CF smiling img2img candidates,\n",
        "    score them with CLIP, keep the top N_KEEP_CF with highest 'smile score'.\n",
        "    \"\"\"\n",
        "    # load + resize source\n",
        "    src_img = Image.open(source_path).convert(\"RGB\")\n",
        "    src_img = src_img.resize((WIDTH, HEIGHT), Image.LANCZOS)\n",
        "\n",
        "    candidates = []\n",
        "\n",
        "    for raw_idx in range(N_RAW_CF):\n",
        "        seed = BASE_SEED + source_id * 1000 + raw_idx\n",
        "        gen = torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "        out = pipe(\n",
        "            prompt=SMILE_PROMPT,\n",
        "            image=src_img,\n",
        "            strength=STRENGTH,\n",
        "            num_inference_steps=NUM_STEPS,\n",
        "            guidance_scale=GUIDANCE,\n",
        "            generator=gen,\n",
        "        )\n",
        "\n",
        "        cf_img = out.images[0]\n",
        "        score = clip_smile_score(cf_img)\n",
        "\n",
        "        candidates.append((raw_idx, seed, score, cf_img))\n",
        "\n",
        "    # sort by CLIP smile score descending\n",
        "    candidates.sort(key=lambda x: x[2], reverse=True)\n",
        "    selected = candidates[:N_KEEP_CF]\n",
        "\n",
        "    entries = []\n",
        "    for rank, (raw_idx, seed, score, cf_img) in enumerate(selected):\n",
        "        fname = f\"src{source_id:03d}_cf{rank:02d}_raw{raw_idx:02d}_seed{seed}_clip.png\"\n",
        "        cf_path = os.path.join(CF_OUT_DIR, fname)\n",
        "        cf_img.save(cf_path)\n",
        "\n",
        "        entries.append(CFEntry(\n",
        "            source_id=source_id,\n",
        "            cf_rank=rank,\n",
        "            raw_idx=raw_idx,\n",
        "            seed=seed,\n",
        "            clip_score=score,\n",
        "            cf_path=cf_path,\n",
        "        ))\n",
        "\n",
        "    return entries\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E8MCMAwck3Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7. Run CLIP sampling for all sources\n",
        "# ============================================================\n",
        "all_entries = []\n",
        "\n",
        "for _, row in tqdm(df_sources.iterrows(), total=len(df_sources), desc=\"Sources\"):\n",
        "    sid  = int(row[\"source_id\"])\n",
        "    spath = row[\"source_path\"]\n",
        "    print(f\"\\n=== Source {sid} ===\")\n",
        "    print(spath)\n",
        "\n",
        "    entries = generate_cfs_for_source(sid, spath)\n",
        "    all_entries.extend(entries)\n",
        "\n",
        "rows = []\n",
        "for e in all_entries:\n",
        "    rows.append({\n",
        "        \"source_id\": e.source_id,\n",
        "        \"cf_rank\": e.cf_rank,\n",
        "        \"raw_idx\": e.raw_idx,\n",
        "        \"seed\": e.seed,\n",
        "        \"clip_smile_score\": e.clip_score,\n",
        "        \"cf_path\": e.cf_path,\n",
        "    })\n",
        "\n",
        "df_cf = pd.DataFrame(rows)\n",
        "df_cf.to_csv(CSV_CF_PATH, index=False)\n",
        "print(\"\\nSaved CLIP-sampled CF metadata to:\", CSV_CF_PATH)\n",
        "display(df_cf.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "uQUoo_2m9zuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8. Quick visual sanity check\n",
        "# ============================================================\n",
        "from IPython.display import display as ipy_display\n",
        "\n",
        "def show_source_and_cfs(source_id: int, max_show: int = 4):\n",
        "    \"\"\"Show one source image and a few of its top CLIP-sampled counterfactuals.\"\"\"\n",
        "    src_row = df_sources.loc[df_sources[\"source_id\"] == source_id].iloc[0]\n",
        "    src_img = Image.open(src_row[\"source_path\"]).convert(\"RGB\").resize((WIDTH, HEIGHT))\n",
        "\n",
        "    print(f\"\\nSource {source_id}:\", src_row[\"source_path\"])\n",
        "    ipy_display(src_img)\n",
        "\n",
        "    sub = df_cf[df_cf[\"source_id\"] == source_id].sort_values(\"cf_rank\").head(max_show)\n",
        "    for _, r in sub.iterrows():\n",
        "        cf_img = Image.open(r[\"cf_path\"]).convert(\"RGB\")\n",
        "        print(f\"CF rank={r['cf_rank']}  score={r['clip_smile_score']:.3f}\")\n",
        "        ipy_display(cf_img)\n",
        "\n",
        "# Example usage:\n",
        "show_source_and_cfs(0, max_show=4)"
      ],
      "metadata": {
        "id": "GylIiRp29Fh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See a few sources + their top smiling CFs\n",
        "show_source_and_cfs(0, max_show=6)\n",
        "show_source_and_cfs(1, max_show=6)\n",
        "show_source_and_cfs(2, max_show=6)\n"
      ],
      "metadata": {
        "id": "gSmGTztKr7xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "AZyPHrbiyF-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q diffusers==0.36.0 transformers lpips scikit-image"
      ],
      "metadata": {
        "id": "IU4FLA_vRzRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, itertools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import lpips\n",
        "\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "QxlEUI1UyI5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR     = \"/content/drive/MyDrive/thesis2\"\n",
        "CF_CSV_PATH  = f\"{BASE_DIR}/phase4_clip_sampling_metadata.csv\"\n",
        "\n",
        "\n",
        "SOURCES_DIR  = f\"{BASE_DIR}/classifier_dataset/neutral\"\n",
        "\n",
        "\n",
        "CSV_PER_PATH      = f\"{BASE_DIR}/phase4_cf_eval_per_image.csv\"\n",
        "CSV_PAIRWISE_PATH = f\"{BASE_DIR}/phase4_cf_eval_pairwise.csv\"\n"
      ],
      "metadata": {
        "id": "pfl7fOHgyOV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_paths = sorted(\n",
        "    glob.glob(os.path.join(SOURCES_DIR, \"*.png\")) +\n",
        "    glob.glob(os.path.join(SOURCES_DIR, \"*.jpg\")) +\n",
        "    glob.glob(os.path.join(SOURCES_DIR, \"*.jpeg\"))\n",
        ")\n",
        "\n",
        "df_sources = pd.DataFrame({\n",
        "    \"source_id\": list(range(len(source_paths))),\n",
        "    \"source_path\": source_paths,\n",
        "})\n",
        "print(\"Sources:\")\n",
        "df_sources.head()\n"
      ],
      "metadata": {
        "id": "woR73yqXyWv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cf = pd.read_csv(CF_CSV_PATH)\n",
        "df_cf.head()\n"
      ],
      "metadata": {
        "id": "UfjjnBSEyZzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/thesis2\"\n",
        "os.makedirs(ROOT, exist_ok=True)\n",
        "CLASSIFIER_CKPT = f\"{ROOT}/models/smile_classifier_best.pt\"\n",
        "\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "clf_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # NO central cropping\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "\n",
        "class SmileResNet18(nn.Module):\n",
        "    def __init__(self, num_classes: int = 2):\n",
        "        super().__init__()\n",
        "        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        in_features = backbone.fc.in_features\n",
        "        backbone.fc = nn.Linear(in_features, num_classes)\n",
        "        self.backbone = backbone\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "# build model on device\n",
        "classifier = SmileResNet18(num_classes=2).to(device)\n",
        "\n",
        "# load checkpoint onto same device\n",
        "state = torch.load(CLASSIFIER_CKPT, map_location=device)\n",
        "missing, unexpected = classifier.load_state_dict(state, strict=False)\n",
        "print(\"Loaded classifier checkpoint.\")\n",
        "print(\"Missing keys:\", missing)\n",
        "print(\"Unexpected keys:\", unexpected)\n",
        "\n",
        "classifier.eval()\n",
        "print(\"Classifier param device:\", next(classifier.parameters()).device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def classifier_prob_smile(pil_img: Image.Image) -> float:\n",
        "    \"\"\"\n",
        "    pil_img: PIL RGB image\n",
        "    Returns: probability of class 'smiling' (index 1)\n",
        "    \"\"\"\n",
        "    x = clf_transform(pil_img).unsqueeze(0).to(device)\n",
        "    logits = classifier(x)  # <-- use `classifier`, not `clf`\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    return float(probs[0, 1].item())\n"
      ],
      "metadata": {
        "id": "k-2U4NPyyqec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model_id = \"openai/clip-vit-base-patch32\"\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_id).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_id)\n",
        "clip_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def clip_image_embedding(pil_img: Image.Image) -> torch.Tensor:\n",
        "    inputs = clip_processor(images=pil_img, return_tensors=\"pt\").to(device)\n",
        "    img_feats = clip_model.get_image_features(**inputs)\n",
        "    img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)\n",
        "    return img_feats.squeeze(0).float()\n"
      ],
      "metadata": {
        "id": "LIZvEZKD1EB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lpips_model = lpips.LPIPS(net=\"vgg\").to(device)\n",
        "\n",
        "def pil_to_lpips_tensor(img: Image.Image) -> torch.Tensor:\n",
        "    t = transforms.ToTensor()(img).unsqueeze(0) * 2 - 1  # [0,1] -> [-1,1]\n",
        "    return t.to(device)\n",
        "\n",
        "def mse_numpy(a, b) -> float:\n",
        "    return float(((a.astype(np.float32) - b.astype(np.float32)) ** 2).mean())\n",
        "\n",
        "def ssim_numpy(a, b) -> float:\n",
        "    # a,b: HxWxC\n",
        "    if a.ndim == 3 and a.shape[2] == 3:\n",
        "        ssim_val = ssim(\n",
        "            a, b,\n",
        "            channel_axis=2,\n",
        "            data_range=255,\n",
        "        )\n",
        "    else:\n",
        "        ssim_val = ssim(a, b, data_range=255)\n",
        "    return float(ssim_val)\n"
      ],
      "metadata": {
        "id": "UEukU_nA1HIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = \"/content/drive/MyDrive/thesis2\"\n",
        "CSV_PER_PATH = f\"{BASE_DIR}/phase4_cf_eval_per_image.csv\"\n"
      ],
      "metadata": {
        "id": "JAIdMYslTbaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WIDTH, HEIGHT = 768, 768  # same as generation\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_counterfactuals_vs_source(df_sources, df_cf) -> pd.DataFrame:\n",
        "    # Build quick lookup: source_id -> path\n",
        "    src_map = {\n",
        "        int(row[\"source_id\"]): row[\"source_path\"]\n",
        "        for _, row in df_sources.iterrows()\n",
        "    }\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for _, r in tqdm(df_cf.iterrows(), total=len(df_cf), desc=\"Evaluating CFs vs sources\"):\n",
        "        src_id = int(r[\"source_id\"])\n",
        "        cf_path = r[\"cf_path\"]\n",
        "\n",
        "        src_path = src_map[src_id]\n",
        "\n",
        "        # Load images\n",
        "        src_img = Image.open(src_path).convert(\"RGB\")\n",
        "        cf_img  = Image.open(cf_path).convert(\"RGB\")\n",
        "\n",
        "        # Resize\n",
        "        src_res = src_img.resize((WIDTH, HEIGHT), Image.LANCZOS)\n",
        "        cf_res  = cf_img.resize((WIDTH, HEIGHT), Image.LANCZOS)\n",
        "\n",
        "        # 1) classifier on CF\n",
        "        p_smile = classifier_prob_smile(cf_res)\n",
        "        label   = int(p_smile >= 0.5)\n",
        "\n",
        "        # 2) LPIPS\n",
        "        t0 = pil_to_lpips_tensor(src_res)\n",
        "        t1 = pil_to_lpips_tensor(cf_res)\n",
        "        lpips_val = float(lpips_model(t0, t1).item())\n",
        "\n",
        "        # 3) MSE + SSIM\n",
        "        np0 = np.array(src_res)\n",
        "        np1 = np.array(cf_res)\n",
        "        mse_val  = mse_numpy(np0, np1)\n",
        "        ssim_val = ssim_numpy(np0, np1)\n",
        "\n",
        "        # 4) CLIP similarity\n",
        "        emb0 = clip_image_embedding(src_res)\n",
        "        emb1 = clip_image_embedding(cf_res)\n",
        "        clip_sim = float((emb0 * emb1).sum().item())\n",
        "\n",
        "        rows.append({\n",
        "            \"source_id\": src_id,\n",
        "            \"cf_rank\": r[\"cf_rank\"],\n",
        "            \"raw_idx\": r[\"raw_idx\"],\n",
        "            \"seed\": r[\"seed\"],\n",
        "            \"cf_path\": cf_path,\n",
        "            \"clip_smile_score_gen\": r[\"clip_smile_score\"],  # from generation stage\n",
        "            \"clf_prob_smile_cf\": p_smile,\n",
        "            \"clf_label_cf\": label,\n",
        "            \"lpips_to_source\": lpips_val,\n",
        "            \"mse_to_source\": mse_val,\n",
        "            \"ssim_to_source\": ssim_val,\n",
        "            \"clip_sim_to_source\": clip_sim,\n",
        "        })\n",
        "\n",
        "    df_eval = pd.DataFrame(rows)\n",
        "    df_eval.to_csv(CSV_PER_PATH, index=False)\n",
        "    print(\"Saved per-image CF eval to:\", CSV_PER_PATH)\n",
        "    return df_eval\n",
        "\n",
        "df_cf_eval = evaluate_counterfactuals_vs_source(df_sources, df_cf)\n",
        "df_cf_eval.head()\n"
      ],
      "metadata": {
        "id": "wJff5xAO1KZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cf_eval = evaluate_counterfactuals_vs_source(df_sources, df_cf)\n"
      ],
      "metadata": {
        "id": "2Y04osXsTitB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"======== COUNTERFACTUAL VALIDITY ========\")\n",
        "total = len(df_cf_eval)\n",
        "flipped = (df_cf_eval[\"clf_label_cf\"] == 1).sum()\n",
        "flip_rate = flipped / total\n",
        "\n",
        "print(f\"Total counterfactuals: {total}\")\n",
        "print(f\"Smiling according to classifier: {flipped}\")\n",
        "print(f\"Decision flip success rate: {flip_rate:.4f}\")\n"
      ],
      "metadata": {
        "id": "PXrSi8-kTlPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n======== PROXIMITY TO SOURCE ========\")\n",
        "prox_stats = df_cf_eval[[\n",
        "    \"lpips_to_source\",\n",
        "    \"ssim_to_source\",\n",
        "    \"mse_to_source\",\n",
        "    \"clip_sim_to_source\"\n",
        "]].describe()\n",
        "\n",
        "print(prox_stats)\n"
      ],
      "metadata": {
        "id": "SPgwsnqnT4Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(df_cf_eval[\"clf_prob_smile_cf\"], bins=25)\n",
        "plt.title(\"Classifier smile probabilities\")\n",
        "plt.xlabel(\"P(smile)\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Uf3V9yFVUkqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.hist(df_cf_eval[\"lpips_to_source\"], bins=25)\n",
        "plt.title(\"LPIPS distance to source\")\n",
        "plt.xlabel(\"LPIPS\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qkzaQnfkUCNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving summaries"
      ],
      "metadata": {
        "id": "J0rU8MX9T7iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_validity = pd.DataFrame([{\n",
        "    \"total_cfs\": total,\n",
        "    \"successful_flips\": flipped,\n",
        "    \"flip_rate\": flip_rate\n",
        "}])\n",
        "\n",
        "summary_validity_path = f\"{BASE_DIR}/phase4_cf_summary_validity.csv\"\n",
        "summary_validity.to_csv(summary_validity_path, index=False)\n",
        "print(\"Saved:\", summary_validity_path)\n"
      ],
      "metadata": {
        "id": "NNOL8XWcT-Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prox_stats_path = f\"{BASE_DIR}/phase4_cf_summary_proximity.csv\"\n",
        "prox_stats.to_csv(prox_stats_path)\n",
        "print(\"Saved:\", prox_stats_path)\n"
      ],
      "metadata": {
        "id": "hJp8p5tXUAC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PHASE IMAGE TABLE BUILDER (Drive scanner)\n",
        "# ============================================\n",
        "import os, re, glob\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG: edit these paths\n",
        "# ----------------------------\n",
        "ROOT = \"/content/drive/MyDrive/thesis2/phase1_smile/images\"\n",
        "\n",
        "PATHS = {\n",
        "    # Synthetic dataset (you created)\n",
        "    \"dataset_neutral\": f\"{ROOT}/neutral\",\n",
        "    \"dataset_smiling\": f\"{ROOT}/smiling\",\n",
        "\n",
        "    # Phase 4 sources (if you used them; else ignore)\n",
        "    #\"phase1_sources_neutral\": f\"{ROOT}/custom_sources\",\n",
        "\n",
        "    # Phase 4 counterfactuals (where you saved CF images)\n",
        "    #\"phase1_counterfactuals\": f\"{ROOT}/phase4_clip_sampling_images\",\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "OUT_DIR = f\"{ROOT}/tables\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "OUT_IMAGES_TABLE_CSV = f\"{OUT_DIR}/images_index_all.csv\"\n",
        "OUT_COUNTS_TABLE_CSV = f\"{OUT_DIR}/images_counts_by_phase_label.csv\"\n",
        "OUT_COUNTS_SRC_CSV   = f\"{OUT_DIR}/images_counts_by_source.csv\"\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def safe_exists(p):\n",
        "    return p and os.path.exists(p)\n",
        "\n",
        "def parse_ids_from_filename(fname: str):\n",
        "    \"\"\"\n",
        "    Try to parse common patterns you used:\n",
        "    - dataset: neutral_0007_p1.png, smiling_0043_p2.png\n",
        "    - counterfactuals: src000_cf01_seed98766.png or source0_cf0_seed123.png etc\n",
        "    \"\"\"\n",
        "    base = os.path.basename(fname)\n",
        "\n",
        "    out = {\n",
        "        \"label_or_cond\": None,\n",
        "        \"index\": None,\n",
        "        \"prompt_variant\": None,\n",
        "        \"source_id\": None,\n",
        "        \"cf_id\": None,\n",
        "        \"seed\": None,\n",
        "    }\n",
        "\n",
        "    # dataset pattern: label_0007_p1.png\n",
        "    m = re.match(r\"^(neutral|smiling|soft_smile|big_smile)_(\\d+)_p(\\d+)\\.(png|jpg|jpeg)$\", base, re.IGNORECASE)\n",
        "    if m:\n",
        "        out[\"label_or_cond\"] = m.group(1).lower()\n",
        "        out[\"index\"] = int(m.group(2))\n",
        "        out[\"prompt_variant\"] = int(m.group(3))\n",
        "        return out\n",
        "\n",
        "    # cf pattern: src000_cf01_seed98766.png\n",
        "    m = re.search(r\"src(\\d+)\", base, re.IGNORECASE)\n",
        "    if m:\n",
        "        out[\"source_id\"] = int(m.group(1))\n",
        "\n",
        "    m = re.search(r\"cf(\\d+)\", base, re.IGNORECASE)\n",
        "    if m:\n",
        "        out[\"cf_id\"] = int(m.group(1))\n",
        "\n",
        "    m = re.search(r\"seed(\\d+)\", base, re.IGNORECASE)\n",
        "    if m:\n",
        "        out[\"seed\"] = int(m.group(1))\n",
        "\n",
        "    # If filename includes \"neutral\"/\"smiling\"\n",
        "    if re.search(\"neutral\", base, re.IGNORECASE):\n",
        "        out[\"label_or_cond\"] = \"neutral\"\n",
        "    if re.search(\"smil\", base, re.IGNORECASE):\n",
        "        out[\"label_or_cond\"] = \"smiling\"\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def scan_folder(phase_name: str, folder: str):\n",
        "    exts = (\"*.png\", \"*.jpg\", \"*.jpeg\", \"*.webp\")\n",
        "    files = []\n",
        "    for ext in exts:\n",
        "        files.extend(glob.glob(os.path.join(folder, ext)))\n",
        "    files = sorted(files)\n",
        "\n",
        "    rows = []\n",
        "    for fp in files:\n",
        "        st = os.stat(fp)\n",
        "        meta = parse_ids_from_filename(fp)\n",
        "\n",
        "        rows.append({\n",
        "            \"phase\": phase_name,\n",
        "            \"folder\": folder,\n",
        "            \"filename\": os.path.basename(fp),\n",
        "            \"filepath\": fp,\n",
        "            \"label_or_cond\": meta[\"label_or_cond\"],\n",
        "            \"index\": meta[\"index\"],\n",
        "            \"prompt_variant\": meta[\"prompt_variant\"],\n",
        "            \"source_id\": meta[\"source_id\"],\n",
        "            \"cf_id\": meta[\"cf_id\"],\n",
        "            \"seed\": meta[\"seed\"],\n",
        "            \"filesize_kb\": round(st.st_size / 1024, 2),\n",
        "            \"modified_time\": datetime.fromtimestamp(st.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def preview_grid(df, title, n=12, seed=0):\n",
        "    \"\"\"\n",
        "    Show a small grid of random images from a filtered dataframe.\n",
        "    \"\"\"\n",
        "    if len(df) == 0:\n",
        "        print(f\"[preview_grid] No images for: {title}\")\n",
        "        return\n",
        "\n",
        "    sample = df.sample(min(n, len(df)), random_state=seed).reset_index(drop=True)\n",
        "    cols = 5\n",
        "    rows = (len(sample) + cols - 1) // cols\n",
        "\n",
        "    plt.figure(figsize=(cols * 3, rows * 3))\n",
        "    plt.suptitle(title, y=1.02)\n",
        "\n",
        "\n",
        "    for i, r in sample.iterrows():\n",
        "        ax = plt.subplot(rows, cols, i+1)\n",
        "        img = Image.open(r[\"filepath\"]).convert(\"RGB\")\n",
        "        ax.imshow(img)\n",
        "        #      ax.set_title(r[\"filename\"][:22], fontsize=8)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Build master table\n",
        "# ----------------------------\n",
        "all_rows = []\n",
        "missing = []\n",
        "\n",
        "for phase_name, folder in PATHS.items():\n",
        "    if safe_exists(folder):\n",
        "        all_rows.extend(scan_folder(phase_name, folder))\n",
        "    else:\n",
        "        missing.append((phase_name, folder))\n",
        "\n",
        "df_images = pd.DataFrame(all_rows)\n",
        "\n",
        "print(\"Total images indexed:\", len(df_images))\n",
        "if missing:\n",
        "    print(\"\\n Missing folders (ignored):\")\n",
        "    for k, p in missing:\n",
        "        print(f\" - {k}: {p}\")\n",
        "\n",
        "# Save master table\n",
        "df_images.to_csv(OUT_IMAGES_TABLE_CSV, index=False)\n",
        "print(\"\\nSaved:\", OUT_IMAGES_TABLE_CSV)\n",
        "\n",
        "# ----------------------------\n",
        "# Summary tables\n",
        "# ----------------------------\n",
        "# 1) counts per phase x label/cond\n",
        "df_counts = (\n",
        "    df_images\n",
        "    .assign(label_or_cond=df_images[\"label_or_cond\"].fillna(\"unknown\"))\n",
        "    .groupby([\"phase\", \"label_or_cond\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"n_images\")\n",
        "    .sort_values([\"phase\", \"label_or_cond\"])\n",
        ")\n",
        "df_counts.to_csv(OUT_COUNTS_TABLE_CSV, index=False)\n",
        "print(\"Saved:\", OUT_COUNTS_TABLE_CSV)\n",
        "\n",
        "# 2) counts per source_id (useful for CF runs)\n",
        "df_by_source = (\n",
        "    df_images[df_images[\"source_id\"].notna()]\n",
        "    .groupby([\"phase\", \"source_id\", \"label_or_cond\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"n_images\")\n",
        "    .sort_values([\"phase\", \"source_id\"])\n",
        ")\n",
        "df_by_source.to_csv(OUT_COUNTS_SRC_CSV, index=False)\n",
        "print(\"Saved:\", OUT_COUNTS_SRC_CSV)\n",
        "\n",
        "display(df_images.head(10))\n",
        "display(df_counts)\n",
        "\n",
        "# ----------------------------\n",
        "# Optional: visual sanity previews\n",
        "# ----------------------------\n",
        "# Dataset previews\n",
        "#preview_grid(df_images[df_images[\"phase\"]==\"dataset_neutral\"], \"Phase 1 neutral (sample)\", n=5, seed=1)\n",
        "preview_grid(df_images[df_images[\"phase\"]==\"dataset_smiling\"], \"Phase 2 smiling (sample)\", n=5, seed=2)\n",
        "\n",
        "# CF preview (if present)\n",
        "#preview_grid(df_images[df_images[\"phase\"].str.contains(\"phase4_counterfactuals\")], \"Phase4 counterfactuals (sample)\", n=30, seed=3)\n",
        "#preview_grid(df_images[df_images[\"phase\"].str.contains(\"phase4_sources_neutral\")], \"Phase4 counterfactuals (sample)\", n=5, seed=3)\n"
      ],
      "metadata": {
        "id": "Bc2bxeXeVLFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_neutral_plus_cf_folder(\n",
        "    neutral_path: str,\n",
        "    cf_folder: str,\n",
        "    cf_glob: str = \"*.png\",\n",
        "    cols_cf: int = 6,\n",
        "    resize_to=(768, 768),\n",
        "    save_path: str = None,\n",
        "):\n",
        "    assert os.path.exists(neutral_path), f\"Neutral not found: {neutral_path}\"\n",
        "    assert os.path.exists(cf_folder), f\"CF folder not found: {cf_folder}\"\n",
        "\n",
        "    cf_paths = sorted(glob.glob(os.path.join(cf_folder, cf_glob)))\n",
        "    assert len(cf_paths) > 0, f\"No CFs found in {cf_folder}\"\n",
        "\n",
        "    n_cf = len(cf_paths)\n",
        "    cols_cf = min(cols_cf, n_cf)\n",
        "    rows = (n_cf + cols_cf - 1) // cols_cf\n",
        "\n",
        "    plt.figure(figsize=((1 + cols_cf) * 3, rows * 3))\n",
        "\n",
        "    ax0 = plt.subplot(rows, 1 + cols_cf, 1)\n",
        "    ax0.imshow(Image.open(neutral_path).convert(\"RGB\").resize(resize_to))\n",
        "    ax0.axis(\"off\")\n",
        "\n",
        "    for i, p in enumerate(cf_paths):\n",
        "        ax = plt.subplot(rows, 1 + cols_cf, 2 + i)\n",
        "        ax.imshow(Image.open(p).convert(\"RGB\").resize(resize_to))\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=200, bbox_inches=\"tight\")\n",
        "        print(\"Saved:\", save_path)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return cf_paths\n"
      ],
      "metadata": {
        "id": "mcByvklgdz0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neutral_path = \"/content/drive/MyDrive/thesis2/custom_sources/smaira.png\"\n",
        "\n",
        "cf_folder = \"/content/drive/MyDrive/thesis2/fethi\"\n",
        "\n",
        "cf_paths = sorted(glob.glob(os.path.join(cf_folder, \"*.png\")))\n",
        "\n",
        "print(\"Found CF images:\", len(cf_paths))\n",
        "print(cf_paths[:3])  # sanity check\n",
        "\n",
        "show_neutral_and_cfs_panel(\n",
        "    neutral_path=neutral_path,\n",
        "    cf_paths=cf_paths,\n",
        "    cols_cf=6,\n",
        "    save_path=\"/content/drive/MyDrive/thesis2/plots/smairaa.png\"\n",
        ")"
      ],
      "metadata": {
        "id": "vkeJ5_4Hd2iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pairwise comparison"
      ],
      "metadata": {
        "id": "EM9PcUTiVrSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install lpips scikit-image transformers\n"
      ],
      "metadata": {
        "id": "7iB4yke6VzPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/thesis2\"\n",
        "\n",
        "# Use whichever you have (eval has more columns; metadata is enough too)\n",
        "CSV_IN = f\"{ROOT}/phase4_cf_eval_per_image.csv\"   # or phase4_clip_sampling_metadata.csv\n",
        "\n",
        "df = pd.read_csv(CSV_IN)\n",
        "print(\"Loaded:\", df.shape)\n",
        "print(df.columns)\n",
        "\n",
        "\n",
        "\n",
        "# Keep only what we need\n",
        "df = df[[\"source_id\", \"cf_rank\", \"seed\", \"cf_path\"]].copy()\n",
        "df[\"cf_path\"] = df[\"cf_path\"].astype(str)\n",
        "\n",
        "# sanity check: how many files exist?\n",
        "exists = df[\"cf_path\"].apply(os.path.isfile)\n",
        "print(\"CF image files found:\", int(exists.sum()), \"/\", len(df))\n",
        "print(\"Missing examples:\", df.loc[~exists, \"cf_path\"].head(5).tolist())\n"
      ],
      "metadata": {
        "id": "zPooQyGve-SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import lpips\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# LPIPS model (VGG is standard)\n",
        "lpips_model = lpips.LPIPS(net=\"vgg\").to(device).eval()\n",
        "\n",
        "def pil_to_lpips_tensor(pil_img: Image.Image) -> torch.Tensor:\n",
        "    # LPIPS expects [-1,1], shape [1,3,H,W]\n",
        "    x = np.array(pil_img).astype(np.float32) / 255.0\n",
        "    x = torch.from_numpy(x).permute(2,0,1).unsqueeze(0)  # [1,3,H,W]\n",
        "    x = x * 2.0 - 1.0\n",
        "    return x.to(device)\n",
        "\n",
        "def mse_numpy(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.mean((a.astype(np.float32) - b.astype(np.float32)) ** 2))\n",
        "\n",
        "def ssim_numpy(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    # SSIM expects HxWxC, use channel_axis=2\n",
        "    return float(ssim(a, b, channel_axis=2, data_range=255))\n",
        "\n",
        "# ---- Optional CLIP image embeddings ----\n",
        "USE_CLIP = True\n",
        "\n",
        "if USE_CLIP:\n",
        "    from transformers import CLIPProcessor, CLIPModel\n",
        "    clip_id = \"openai/clip-vit-base-patch32\"\n",
        "    clip_model = CLIPModel.from_pretrained(clip_id).to(device).eval()\n",
        "    clip_proc  = CLIPProcessor.from_pretrained(clip_id)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def clip_img_emb(pil_img: Image.Image) -> torch.Tensor:\n",
        "        inp = clip_proc(images=pil_img, return_tensors=\"pt\")\n",
        "        inp = {k:v.to(device) for k,v in inp.items()}\n",
        "        feat = clip_model.get_image_features(**inp)\n",
        "        feat = F.normalize(feat, dim=-1)\n",
        "        return feat.squeeze(0)  # [D]\n"
      ],
      "metadata": {
        "id": "KRYLz-25Wb8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_PAIRS   = f\"{ROOT}/phase4_cf_pairwise_pairs.csv\"\n",
        "OUT_SUMMARY = f\"{ROOT}/phase4_cf_pairwise_summary.csv\"\n",
        "\n",
        "# for consistent resizing (match your generation)\n",
        "WIDTH, HEIGHT = 768, 768\n",
        "\n",
        "def load_img(path: str) -> Image.Image:\n",
        "    return Image.open(path).convert(\"RGB\").resize((WIDTH, HEIGHT), Image.LANCZOS)\n",
        "\n",
        "rows = []\n",
        "\n",
        "for src_id, g in df.groupby(\"source_id\"):\n",
        "    g = g.sort_values(\"cf_rank\").reset_index(drop=True)\n",
        "\n",
        "    paths = g[\"cf_path\"].tolist()\n",
        "    ranks = g[\"cf_rank\"].tolist()\n",
        "\n",
        "    # load once\n",
        "    imgs = [load_img(p) for p in paths]\n",
        "\n",
        "    # precompute clip embeddings once\n",
        "    if USE_CLIP:\n",
        "        embs = [clip_img_emb(im) for im in imgs]\n",
        "\n",
        "    # all unordered pairs\n",
        "    for i, j in itertools.combinations(range(len(imgs)), 2):\n",
        "        im_i, im_j = imgs[i], imgs[j]\n",
        "\n",
        "        # LPIPS\n",
        "        t_i = pil_to_lpips_tensor(im_i)\n",
        "        t_j = pil_to_lpips_tensor(im_j)\n",
        "        lp = float(lpips_model(t_i, t_j).item())\n",
        "\n",
        "        # SSIM / MSE\n",
        "        a = np.array(im_i)\n",
        "        b = np.array(im_j)\n",
        "        mse_val = mse_numpy(a, b)\n",
        "        ssim_val = ssim_numpy(a, b)\n",
        "\n",
        "        # CLIP distance\n",
        "        if USE_CLIP:\n",
        "            cos = float((embs[i] * embs[j]).sum().item())\n",
        "            clip_dist = float(1.0 - cos)\n",
        "        else:\n",
        "            cos, clip_dist = np.nan, np.nan\n",
        "\n",
        "        rows.append({\n",
        "            \"source_id\": int(src_id),\n",
        "            \"cf_rank_i\": int(ranks[i]),\n",
        "            \"cf_rank_j\": int(ranks[j]),\n",
        "            \"cf_path_i\": paths[i],\n",
        "            \"cf_path_j\": paths[j],\n",
        "            \"lpips\": lp,\n",
        "            \"ssim\": ssim_val,\n",
        "            \"mse\": mse_val,\n",
        "            \"clip_cos\": cos,\n",
        "            \"clip_dist\": clip_dist,\n",
        "        })\n",
        "\n",
        "df_pairs = pd.DataFrame(rows)\n",
        "df_pairs.to_csv(OUT_PAIRS, index=False)\n",
        "print(\"Saved pairs:\", OUT_PAIRS, \" shape:\", df_pairs.shape)\n",
        "\n",
        "# summary per source\n",
        "agg_cols = [\"lpips\", \"ssim\", \"mse\"] + ([\"clip_dist\"] if USE_CLIP else [])\n",
        "df_summary = (\n",
        "    df_pairs\n",
        "    .groupby(\"source_id\")[agg_cols]\n",
        "    .agg([\"mean\", \"std\", \"count\"])\n",
        "    .reset_index()\n",
        ")\n",
        "df_summary.columns = [\"_\".join([c for c in col if c]).strip(\"_\") for col in df_summary.columns.values]\n",
        "df_summary.to_csv(OUT_SUMMARY, index=False)\n",
        "print(\"Saved summary:\", OUT_SUMMARY, \" shape:\", df_summary.shape)\n",
        "\n",
        "display(df_summary)\n"
      ],
      "metadata": {
        "id": "63GGPon7WeIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# baseline"
      ],
      "metadata": {
        "id": "LOOM7Lyz6y-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Standard DDPM-style counterfactual baseline (no CLIP ranking)\n",
        "# SD3.5 Img2Img: generate K smile candidates per source by seed variation\n",
        "# Evaluate: validity (classifier flip), proximity-to-source, within-set diversity\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from diffusers import AutoPipelineForImage2Image\n",
        "from diffusers.utils import load_image\n",
        "\n",
        "# ---------------------------\n",
        "# 0) Set paths + device\n",
        "# ---------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "\n",
        "\n",
        "MODEL_ID = \"stabilityai/stable-diffusion-3.5-medium\"\n",
        "\n",
        "\n",
        "SOURCES_DIR = \"/content/drive/MyDrive/thesis2/custom_sources\"\n",
        "OUT_DIR     = \"/content/drive/MyDrive/thesis2/baseline_ddpm_cf\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Load SD3 Img2Img pipeline\n",
        "# ---------------------------\n",
        "pipe = AutoPipelineForImage2Image.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=dtype,\n",
        "    variant=\"fp16\" if dtype == torch.float16 else None,\n",
        ")\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Prompts + generation params\n",
        "# ---------------------------\n",
        "PROMPT_SMILE = (\n",
        "    \"a photorealistic human face, studio lighting, high resolution, realistic anatomy, \"\n",
        "    \"smiling, natural smile, subtle teeth visibility, joyful expression\"\n",
        ")\n",
        "\n",
        "NEG_PROMPT = (\n",
        "    \"cartoon, illustration, deformed face, disfigured, extra limbs, bad anatomy, \"\n",
        "    \"weird teeth, exaggerated grin, low quality, blurry\"\n",
        ")\n",
        "\n",
        "# Img2img knobs\n",
        "CFG_SCALE   = 7.5\n",
        "STRENGTH    = 0.60        # how strong the edit is\n",
        "STEPS       = 30\n",
        "K           = 6           # number of CFs per source (like your Phase 4)\n",
        "SEED0       = 1000        # base seed (repeatability)\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Utility: list source images\n",
        "# ---------------------------\n",
        "def list_images(folder: str) -> List[str]:\n",
        "    exts = {\".png\", \".jpg\", \".jpeg\", \".webp\"}\n",
        "    paths = []\n",
        "    for p in sorted(Path(folder).iterdir()):\n",
        "        if p.suffix.lower() in exts:\n",
        "            paths.append(str(p))\n",
        "    return paths\n",
        "\n",
        "source_paths = list_images(SOURCES_DIR)\n",
        "print(f\"Found {len(source_paths)} neutral sources in {SOURCES_DIR}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Plug in smile classifier (must return prob_smile in [0,1])\n",
        "#\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def predict_smile_prob(pil_img: Image.Image) -> float:\n",
        "\n",
        "\n",
        "    raise NotImplementedError(\"Replace predict_smile_prob() with your classifier inference.\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Metrics: MSE, SSIM (simple), LPIPS (optional)\n",
        "# ---------------------------\n",
        "def pil_to_torch(img: Image.Image, size: Tuple[int,int]=None) -> torch.Tensor:\n",
        "    if size is not None:\n",
        "        img = img.resize(size, Image.BICUBIC)\n",
        "    arr = np.asarray(img).astype(np.float32) / 255.0\n",
        "    if arr.ndim == 2:\n",
        "        arr = np.stack([arr]*3, axis=-1)\n",
        "    t = torch.from_numpy(arr).permute(2,0,1).unsqueeze(0)  # 1x3xHxW\n",
        "    return t\n",
        "\n",
        "@torch.no_grad()\n",
        "def mse(a: Image.Image, b: Image.Image) -> float:\n",
        "    size = (512, 512)\n",
        "    ta = pil_to_torch(a, size=size)\n",
        "    tb = pil_to_torch(b, size=size)\n",
        "    return F.mse_loss(ta, tb).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ssim_simple(a: Image.Image, b: Image.Image) -> float:\n",
        "    size = (512, 512)\n",
        "    x = pil_to_torch(a, size=size)\n",
        "    y = pil_to_torch(b, size=size)\n",
        "    # luminance-only quick SSIM-ish:\n",
        "    xg = x.mean(dim=1, keepdim=True)\n",
        "    yg = y.mean(dim=1, keepdim=True)\n",
        "    mu_x = xg.mean()\n",
        "    mu_y = yg.mean()\n",
        "    sig_x = xg.var(unbiased=False)\n",
        "    sig_y = yg.var(unbiased=False)\n",
        "    sig_xy = ((xg - mu_x) * (yg - mu_y)).mean()\n",
        "    C1, C2 = 0.01**2, 0.03**2\n",
        "    ssim_val = ((2*mu_x*mu_y + C1) * (2*sig_xy + C2)) / ((mu_x**2 + mu_y**2 + C1) * (sig_x + sig_y + C2))\n",
        "    return float(ssim_val.clamp(-1, 1).item())\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def lpips_stub(a: Image.Image, b: Image.Image) -> float:\n",
        "    return float(\"nan\")\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Generate baseline CFs for one source (seed diversity only)\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def generate_baseline_cfs_for_source(\n",
        "    source_img: Image.Image,\n",
        "    source_id: str,\n",
        "    k: int = 6,\n",
        "    seed0: int = 0,\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Returns list of dicts with paths + metadata.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for i in range(k):\n",
        "        seed = seed0 + i\n",
        "        gen = torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "        out = pipe(\n",
        "            prompt=PROMPT_SMILE,\n",
        "            negative_prompt=NEG_PROMPT,\n",
        "            image=source_img,\n",
        "            strength=STRENGTH,\n",
        "            guidance_scale=CFG_SCALE,\n",
        "            num_inference_steps=STEPS,\n",
        "            generator=gen,\n",
        "        ).images[0]\n",
        "\n",
        "        out_path = os.path.join(OUT_DIR, f\"{source_id}_cf_{i:02d}_seed{seed}.png\")\n",
        "        out.save(out_path)\n",
        "\n",
        "        rows.append({\n",
        "            \"source_id\": source_id,\n",
        "            \"cf_idx\": i,\n",
        "            \"seed\": seed,\n",
        "            \"cf_path\": out_path,\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "# ---------------------------\n",
        "# 7) Evaluate proximity + validity for baseline CFs\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate_source_to_cf(source_img: Image.Image, cf_img: Image.Image) -> Dict:\n",
        "    return {\n",
        "        \"mse_to_source\": mse(source_img, cf_img),\n",
        "        \"ssim_to_source\": ssim_simple(source_img, cf_img),\n",
        "        \"lpips_to_source\": lpips_stub(source_img, cf_img),\n",
        "        # plus classifier validity:\n",
        "        # \"prob_smile\": predict_smile_prob(cf_img),\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# 8) Evaluate within-set diversity among CFs of same source\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate_within_set(cf_imgs: List[Image.Image]) -> Dict:\n",
        "    \"\"\"\n",
        "    Mean over all unordered CF-CF pairs within a set.\n",
        "    \"\"\"\n",
        "    K = len(cf_imgs)\n",
        "    pairs = [(i, j) for i in range(K) for j in range(i+1, K)]\n",
        "    if len(pairs) == 0:\n",
        "        return {}\n",
        "\n",
        "    lp_list, ssim_list, mse_list = [], [], []\n",
        "    for i, j in pairs:\n",
        "        a, b = cf_imgs[i], cf_imgs[j]\n",
        "        mse_list.append(mse(a, b))\n",
        "        ssim_list.append(ssim_simple(a, b))\n",
        "        lp_list.append(lpips_stub(a, b))\n",
        "\n",
        "    return {\n",
        "        \"K\": K,\n",
        "        \"n_pairs\": len(pairs),\n",
        "        \"within_mse_mean\": float(np.mean(mse_list)),\n",
        "        \"within_ssim_mean\": float(np.mean(ssim_list)),\n",
        "        \"within_lpips_mean\": float(np.nanmean(lp_list)),\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# 9) Main loop: generate + evaluate baseline\n",
        "# ---------------------------\n",
        "all_cf_rows = []\n",
        "prox_rows = []\n",
        "within_rows = []\n",
        "\n",
        "for s_idx, sp in enumerate(source_paths):\n",
        "    source_id = Path(sp).stem\n",
        "    src = Image.open(sp).convert(\"RGB\")\n",
        "\n",
        "    # Generate baseline K counterfactuals (seed diversity only)\n",
        "    cf_rows = generate_baseline_cfs_for_source(\n",
        "        source_img=src,\n",
        "        source_id=source_id,\n",
        "        k=K,\n",
        "        seed0=SEED0 + 100*s_idx,  # keep seeds separated per source\n",
        "    )\n",
        "    all_cf_rows.extend(cf_rows)\n",
        "\n",
        "    # Load CF images for evaluation\n",
        "    cf_imgs = [Image.open(r[\"cf_path\"]).convert(\"RGB\") for r in cf_rows]\n",
        "\n",
        "    # Proximity-to-source (source vs each CF)\n",
        "    for r, cf_img in zip(cf_rows, cf_imgs):\n",
        "        metrics = evaluate_source_to_cf(src, cf_img)\n",
        "        prox_rows.append({**r, **metrics})\n",
        "\n",
        "    # Within-set diversity (CF vs CF for same source)\n",
        "    within = evaluate_within_set(cf_imgs)\n",
        "    within_rows.append({\"source_id\": source_id, **within})\n",
        "\n",
        "# Save outputs\n",
        "df_index  = pd.DataFrame(all_cf_rows)\n",
        "df_prox   = pd.DataFrame(prox_rows)\n",
        "df_within = pd.DataFrame(within_rows)\n",
        "\n",
        "index_csv  = os.path.join(OUT_DIR, \"baseline_ddpm_cf_index.csv\")\n",
        "prox_csv   = os.path.join(OUT_DIR, \"baseline_ddpm_proximity.csv\")\n",
        "within_csv = os.path.join(OUT_DIR, \"baseline_ddpm_within_set_diversity.csv\")\n",
        "\n",
        "df_index.to_csv(index_csv, index=False)\n",
        "df_prox.to_csv(prox_csv, index=False)\n",
        "df_within.to_csv(within_csv, index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" -\", index_csv)\n",
        "print(\" -\", prox_csv)\n",
        "print(\" -\", within_csv)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Ec9Doqk-u1x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}